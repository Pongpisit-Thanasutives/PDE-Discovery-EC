{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8401bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from functools import partial\n",
    "from decimal import Decimal\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import pysindy as ps\n",
    "from tqdm import trange\n",
    "\n",
    "from pymoo_ga import *\n",
    "from pymoo.algorithms.moo.nsga2 import NSGA2\n",
    "from pymoo.algorithms.moo.dnsga2 import DNSGA2\n",
    "from pymoo.termination.default import DefaultMultiObjectiveTermination\n",
    "from pymoo.optimize import minimize\n",
    "from pymoo.core.problem import StarmapParallelization\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "from utils import *\n",
    "from skimage.restoration import estimate_sigma\n",
    "import bm3d\n",
    "# from okridge.solvel0 import *\n",
    "from solvel0 import solvel0, MIOSR\n",
    "from best_subset import backward_refinement, brute_force_all_subsets\n",
    "from UBIC import *\n",
    "from kneed import KneeLocator\n",
    "from bayesian_model_evidence import log_evidence\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import covariance\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.linear_model import BayesianRidge, ARDRegression, lars_path\n",
    "from bayesian_linear_regression import BayesianLinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5916f2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_poly = 6\n",
    "n_derivatives = 6\n",
    "n_modules = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eaa96d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../PDE-Discovery-EC/Datasets/\"\n",
    "data = sio.loadmat(os.path.join(data_path, \"burgers.mat\"))\n",
    "u_clean = (data['usol']).real; u = u_clean.copy()\n",
    "x = (data['x'][0]).real\n",
    "t = (data['t'][:,0]).real\n",
    "dt = t[1]-t[0]; dx = x[2]-x[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1e0adc",
   "metadata": {},
   "source": [
    "### Add noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "888ee41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise level: 50.0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "noise_type = \"gaussian\"\n",
    "noise_lv = float(50)\n",
    "print(\"Noise level:\", noise_lv)\n",
    "noise = 0.01*np.abs(noise_lv)*(u.std())*np.random.randn(u.shape[0],u.shape[1])\n",
    "u = u + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c890b0",
   "metadata": {},
   "source": [
    "### Denoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9754901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bm3d_file = f\"./Denoised_data/burgers_{noise_type}{int(noise_lv)}_bm3d.npy\"\n",
    "# load_denoised_data = True\n",
    "# if load_denoised_data:\n",
    "#     print(\"Loading denoised data...\")\n",
    "#     u = np.load(bm3d_file)\n",
    "# else:\n",
    "#     kernel = RBF(length_scale=1, length_scale_bounds=(1e-2, 1e3)) + \\\n",
    "#                     WhiteKernel(noise_level=1, noise_level_bounds=(1e-10, 1e10))\n",
    "    \n",
    "#     xx = colvec(x)\n",
    "#     u_mean = np.copy(u)\n",
    "#     u_std = np.ones(u.shape)\n",
    "#     for i in trange(len(t)):    \n",
    "#         gpr = GaussianProcessRegressor(kernel=kernel, alpha=0.0, \n",
    "#                                        n_restarts_optimizer=10 # 20\n",
    "#                                       )\n",
    "    \n",
    "#         gpr.fit(xx, u_mean[:, i])\n",
    "#         um, ustd = gpr.predict(xx, return_std=True)\n",
    "#         u_mean[:, i] = um\n",
    "#         u_std[:, i] = ustd\n",
    "    \n",
    "#     est_sigma = u_std.mean() # max also works well\n",
    "#     # est_sigma = (est_sigma+estimate_sigma(u))/2\n",
    "#     u = bm3d.bm3d(u, sigma_psd=est_sigma, \n",
    "#                   stage_arg=bm3d.BM3DStages.ALL_STAGES, \n",
    "#                   blockmatches=(False, False))\n",
    "    \n",
    "#     ### to save ###\n",
    "#     # np.save(bm3d_file, u)\n",
    "\n",
    "np.random.seed(0)\n",
    "fake_noise = np.random.normal(loc=0.0, scale=estimate_sigma(u), size=u.shape)\n",
    "sigmas = estimate_sigma(u+fake_noise)*np.arange(0.1, 2., 0.1)\n",
    "est_sigma = sigmas[np.argmin([((u-bm3d.bm3d(u+fake_noise, sigma_psd=sigma, stage_arg=bm3d.BM3DStages.ALL_STAGES, blockmatches=(False, False)))**2).mean() \\\n",
    "                              for sigma in sigmas])]\n",
    "u = bm3d.bm3d(u, sigma_psd=est_sigma, \n",
    "                  stage_arg=bm3d.BM3DStages.ALL_STAGES, \n",
    "                  blockmatches=(False, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9921db4e-3de7-42e2-836a-64a85cea979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = np.array([x.reshape(-1, 1), t.reshape(1, -1)], dtype=object)\n",
    "X, T = np.meshgrid(x, t)\n",
    "XT = np.asarray([X, T]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aaf2666",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_library = ps.PolynomialLibrary(degree=n_poly, include_bias=False)\n",
    "\n",
    "weak_lib = ps.WeakPDELibrary(\n",
    "    function_library=function_library,\n",
    "    derivative_order=n_derivatives,\n",
    "    spatiotemporal_grid=XT,\n",
    "    include_bias=True,\n",
    "    diff_kwargs={\"is_uniform\":True},\n",
    "    K=10000\n",
    ")\n",
    "\n",
    "X_pre = np.array(weak_lib.fit_transform(np.expand_dims(u, -1)))\n",
    "y_pre = weak_lib.convert_u_dot_integral(np.expand_dims(u, -1))\n",
    "feature_names = np.array(weak_lib.get_feature_names())\n",
    "\n",
    "# R_path = \"./Cache/\"\n",
    "# np.save(os.path.join(R_path, f\"X_pre_burgers_noise{int(noise_lv)}.npy\"), X_pre)\n",
    "# np.save(os.path.join(R_path, f\"y_pre_burgers_noise{int(noise_lv)}.npy\"), y_pre)\n",
    "# np.save(os.path.join(R_path, f\"feature_names_burgers.npy\"), feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ad9a594",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_poly = np.array([[p, 0] for p in range(1, n_poly+1)])\n",
    "base_derivative = np.array([[0, d] for d in range(1, n_derivatives+1)])\n",
    "modules = [(0, 0)] if weak_lib.include_bias else []\n",
    "modules += [(p, 0) for p in range(1, n_poly+1)] + \\\n",
    "            [(0, d) for d in range(1, n_derivatives+1)] + \\\n",
    "            [tuple(p+d) for d in base_derivative for p in base_poly]\n",
    "assert len(modules) == len(weak_lib.get_feature_names())\n",
    "base_features = dict(zip(modules, X_pre.T))\n",
    "u_t = y_pre.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bede4c-0838-41ab-9dcb-629f03e4f564",
   "metadata": {},
   "source": [
    "### Straightforward best-subset selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acf9148f-099b-4576-872a-50180f02dbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# over 30 minutes for n_poly = 6 and n_derivatives = 6\n",
    "# miosr_subsets = solvel0(X_pre, y_pre, miosr=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb73e146",
   "metadata": {},
   "source": [
    "### Genetic algorithm with NSGA-II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "263f3258",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_size = 500\n",
    "order_complexity = False\n",
    "pool = ThreadPool(4)\n",
    "problem = PdeDiscoveryProblem(n_poly, n_derivatives, n_modules, \n",
    "                              base_features, u_t, order_complexity=order_complexity, ridge_lambda=1e-6, \n",
    "                              elementwise_runner=StarmapParallelization(pool.starmap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "296e4b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prefix(file_path, prefix):\n",
    "    dir_name, file_name = os.path.split(file_path)\n",
    "    return os.path.join(dir_name, prefix + file_name)\n",
    "    \n",
    "load_pareto_front = True\n",
    "n_max_gen = 200\n",
    "n_max_evals = 100000\n",
    "pf_file_path = f\"./Cache/pf_SMSEMOA_burgers_noise{int(noise_lv)}.npy\"\n",
    "\n",
    "if not load_pareto_front:\n",
    "    termination = DefaultMultiObjectiveTermination(\n",
    "        xtol=1e-8,\n",
    "        cvtol=1e-6,\n",
    "        ftol=1e-8,\n",
    "        period=50,\n",
    "        n_max_gen=n_max_gen,\n",
    "        n_max_evals=n_max_evals\n",
    "    )\n",
    "    \n",
    "    from pymoo.algorithms.moo.sms import SMSEMOA\n",
    "    from pymoo.algorithms.moo.age import AGEMOEA\n",
    "    \n",
    "    ### Optimization algorithms ###\n",
    "    # algorithm = NSGA2(pop_size=pop_size,\n",
    "    #                     sampling=PopulationSampling(),\n",
    "    #                     crossover=GenomeCrossover(),\n",
    "    #                     mutation=GenomeMutation(),\n",
    "    #                     eliminate_duplicates=DuplicateElimination(),\n",
    "    #                     )\n",
    "    \n",
    "    # algorithm = DNSGA2(pop_size=pop_size,\n",
    "    #                 sampling=PopulationSampling(),\n",
    "    #                 crossover=GenomeCrossover(),\n",
    "    #                 mutation=GenomeMutation(),\n",
    "    #                 eliminate_duplicates=DuplicateElimination(),\n",
    "    #                 )\n",
    "    \n",
    "    algorithm = SMSEMOA(pop_size=pop_size,\n",
    "                    sampling=PopulationSampling(),\n",
    "                    crossover=GenomeCrossover(),\n",
    "                    mutation=GenomeMutation(),\n",
    "                    eliminate_duplicates=DuplicateElimination(),\n",
    "                    )\n",
    "\n",
    "    opt_time = time.time()\n",
    "    res = minimize(problem, \n",
    "                   algorithm, \n",
    "                   termination=termination, \n",
    "                   verbose=True)\n",
    "    opt_time = (time.time() - opt_time)/60\n",
    "    print(\"Execution time:\", opt_time)\n",
    "    \n",
    "    pareto_optimal_models = res.X\n",
    "    np.save(pf_file_path, pareto_optimal_models)\n",
    "\n",
    "else:\n",
    "    pareto_optimal_models = np.load(pf_file_path, allow_pickle=True)\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66468fa3-d3a1-45f6-9c8f-d89b0d723713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[frozenset({(0, 1)})],\n",
       "       [frozenset({(1, 1), (0, 2)})],\n",
       "       [frozenset({(1, 1), (0, 2), (0, 4)})],\n",
       "       [frozenset({(1, 1), (0, 2), (0, 4), (4, 3)})],\n",
       "       [frozenset({(0, 4), (1, 1), (0, 2), (3, 6), (6, 3)})],\n",
       "       [frozenset({(0, 4), (1, 1), (0, 6), (0, 2), (3, 6), (6, 3)})],\n",
       "       [frozenset({(0, 4), (2, 1), (1, 1), (0, 6), (0, 2), (3, 6), (6, 3)})],\n",
       "       [frozenset({(0, 4), (3, 4), (1, 1), (4, 6), (5, 4), (0, 2), (5, 6), (5, 3)})],\n",
       "       [frozenset({(6, 2), (1, 2), (0, 4), (1, 1), (0, 6), (0, 2), (5, 3), (3, 2), (5, 2)})],\n",
       "       [frozenset({(6, 2), (1, 2), (0, 4), (5, 5), (1, 1), (0, 6), (0, 2), (5, 3), (3, 2), (5, 2)})],\n",
       "       [frozenset({(6, 2), (1, 2), (0, 4), (2, 1), (1, 1), (4, 2), (0, 6), (0, 2), (3, 6), (5, 3), (3, 2)})],\n",
       "       [frozenset({(6, 2), (1, 2), (0, 4), (2, 1), (5, 5), (1, 1), (0, 3), (4, 2), (0, 6), (0, 2), (3, 2), (6, 3)})],\n",
       "       [frozenset({(2, 4), (1, 2), (0, 4), (6, 2), (6, 1), (1, 1), (0, 3), (5, 4), (0, 6), (0, 2), (5, 0), (5, 6), (6, 0)})],\n",
       "       [frozenset({(2, 4), (1, 2), (0, 4), (6, 0), (1, 1), (0, 3), (5, 4), (5, 1), (0, 6), (0, 2), (5, 0), (5, 6), (3, 6), (5, 2)})],\n",
       "       [frozenset({(2, 4), (1, 2), (0, 4), (5, 5), (6, 0), (1, 1), (0, 3), (5, 4), (5, 1), (0, 6), (0, 2), (5, 0), (5, 6), (3, 6), (5, 2)})],\n",
       "       [frozenset({(2, 4), (1, 2), (0, 4), (5, 5), (6, 0), (1, 1), (0, 3), (5, 4), (5, 1), (0, 6), (0, 2), (5, 0), (5, 6), (0, 5), (3, 6), (5, 2)})],\n",
       "       [frozenset({(2, 4), (1, 2), (0, 4), (4, 0), (5, 5), (6, 0), (6, 2), (1, 1), (0, 3), (5, 4), (5, 1), (0, 6), (0, 2), (5, 6), (0, 5), (3, 6), (3, 2)})],\n",
       "       [frozenset({(2, 4), (1, 2), (0, 4), (5, 5), (6, 0), (1, 1), (0, 3), (5, 4), (5, 1), (0, 6), (0, 2), (5, 0), (3, 6), (0, 5), (2, 2), (5, 6), (3, 2), (5, 2)})],\n",
       "       [frozenset({(5, 4), (5, 1), (0, 2), (0, 5), (2, 2), (5, 0), (5, 6), (3, 6), (2, 4), (1, 2), (0, 4), (6, 4), (3, 2), (5, 2), (5, 5), (1, 1), (0, 3), (0, 6), (6, 0)})],\n",
       "       [frozenset({(5, 4), (5, 1), (0, 2), (0, 5), (2, 2), (5, 0), (5, 6), (3, 6), (2, 4), (1, 2), (0, 4), (1, 5), (6, 4), (3, 2), (5, 2), (5, 5), (1, 1), (0, 3), (0, 6), (6, 0)})],\n",
       "       [frozenset({(5, 4), (5, 1), (0, 2), (2, 5), (1, 3), (6, 2), (3, 3), (5, 0), (5, 6), (3, 6), (2, 4), (1, 2), (0, 4), (1, 5), (3, 2), (3, 5), (5, 2), (1, 1), (0, 6), (2, 3), (6, 0)})],\n",
       "       [frozenset({(5, 4), (5, 1), (0, 2), (2, 5), (1, 3), (6, 2), (3, 3), (5, 0), (5, 6), (3, 6), (2, 4), (1, 2), (0, 4), (1, 5), (3, 2), (3, 5), (5, 2), (1, 1), (0, 6), (2, 3), (6, 0), (6, 3)})]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### REFINE PARETO FRONT ###\n",
    "from operator import itemgetter\n",
    "\n",
    "effective_candidates = extract_unique_candidates(pareto_optimal_models)\n",
    "new_pareto_optimal_models = []\n",
    "for bs in backward_refinement([sorted([effective_candidates.index(_) for _ in list(pm[0])]) for pm in pareto_optimal_models], \n",
    "                              (problem.numericalize_genome(effective_candidates), y_pre)).get_best_subsets():\n",
    "    bs = itemgetter(*bs)(effective_candidates)\n",
    "    if type(bs[0]) is not tuple:\n",
    "        bs = (bs,)\n",
    "    new_pareto_optimal_models.append([frozenset(bs)])\n",
    "pareto_optimal_models = np.array(new_pareto_optimal_models)\n",
    "del new_pareto_optimal_models\n",
    "pareto_optimal_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9066caf0-4815-4132-bbc8-6eed80f77d07",
   "metadata": {},
   "source": [
    "### Top candidates by SHAP or Lasso/Lars path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ec2d804-2f67-48dc-8c67-ba0dc23cdcff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1],\n",
       " [1, 1],\n",
       " [0, 2],\n",
       " [6, 3],\n",
       " [0, 4],\n",
       " [3, 6],\n",
       " [1, 4],\n",
       " [2, 6],\n",
       " [0, 3],\n",
       " [0, 6],\n",
       " [1, 3],\n",
       " [1, 2],\n",
       " [5, 5],\n",
       " [6, 0],\n",
       " [6, 2],\n",
       " [2, 0],\n",
       " [1, 5],\n",
       " [5, 3],\n",
       " [4, 1],\n",
       " [3, 2],\n",
       " [6, 5]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature_importance = dict(zip(effective_candidates, [0.0 for _ in range(len(effective_candidates))]))\n",
    "\n",
    "# for bs in pareto_optimal_models[1:]:\n",
    "#     bs = list(bs[0])\n",
    "#     shap_importance = shap_linear_importance(problem.numericalize_genome(bs), y_pre, scale=False)\n",
    "#     for i, _ in enumerate(bs):\n",
    "#         feature_importance[_] += shap_importance[i]\n",
    "\n",
    "# top_candidates = sorted([(v, k) for k, v in feature_importance.items()], reverse=True)\n",
    "# top_candidates = [v for k, v in top_candidates[:16]]\n",
    "\n",
    "_, lars_p, _ = lars_path(StandardScaler().fit_transform(problem.numericalize_genome(effective_candidates)), \n",
    "                         y_pre.ravel(), method='lasso', alpha_min=1e-5)\n",
    "top_candidates = np.array(effective_candidates)[lars_p].tolist()\n",
    "\n",
    "top_candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092f6e10-8c4f-4863-8745-e32b434c6013",
   "metadata": {},
   "source": [
    "### Best-subset selections (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c724e549-426e-4893-a5c6-990e8b0398b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_pre_top = problem.numericalize_genome(top_candidates)\n",
    "# X_pre_top, X_pre_top_norm = normalize_lp(X_pre_top, p=2, axis=0)\n",
    "\n",
    "# best_subsets = solvel0(X_pre_top, y_pre, miosr=True, refine=True)\n",
    "# pareto_optimal_models = [[np.array(top_candidates)[list(bs)]] for bs in best_subsets]\n",
    "\n",
    "# _, _, pde_uncertainties = baye_uncertainties(best_subsets, (X_pre_top, y_pre), \n",
    "#                                              u_type='cv1', take_sqrt=True, \n",
    "#                                              ridge_lambda=0, \n",
    "#                                              threshold=0)\n",
    "\n",
    "# best_subsets, pde_uncertainties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aed3f12-6973-4090-bc95-9d42f3384457",
   "metadata": {},
   "source": [
    "### Uncertainty quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "042cbed9-f030-4ce6-9d86-e9c83d8f16fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 1.94432012e-01, 5.57996688e-03],\n",
       "       [2.00000000e+00, 6.72030745e-02, 3.41602339e-03],\n",
       "       [3.00000000e+00, 6.60818323e-02, 4.54679369e-03],\n",
       "       [4.00000000e+00, 6.53025835e-02, 2.52252763e-02],\n",
       "       [7.00000000e+00, 6.07779740e-02, 7.74642442e-02],\n",
       "       [8.00000000e+00, 6.08256784e-02, 9.17427846e-02],\n",
       "       [1.10000000e+01, 5.64329172e-02, 7.07499281e-02],\n",
       "       [1.30000000e+01, 5.61607957e-02, 6.88625159e-02]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numericalize_genome = False\n",
    "F = {}\n",
    "\n",
    "# for each number of active terms -> keep the best coef (in terms of ssr) and track its uncertainty...\n",
    "for bs in pareto_optimal_models:\n",
    "    numerical_genome = problem.numericalize_genome(bs[0])\n",
    "    if numericalize_genome:\n",
    "        numerical_genome = normalize_lp(numerical_genome)[0]\n",
    "    \n",
    "    # um = BayesianLinearRegression() # seems to work well with numericalize_genome = True\n",
    "    um = ARDRegression(fit_intercept=False, compute_score=True, max_iter=1000)\n",
    "    um.fit(numerical_genome, y_pre.ravel())\n",
    "    \n",
    "    # number of effective parameters\n",
    "    um_n_params = np.count_nonzero(um.coef_)\n",
    "    # SSR\n",
    "    ssr = np.sum((um.predict(numerical_genome) - y_pre.ravel())**2)\n",
    "    # IC\n",
    "    bic = BIC_AIC(um.predict(numerical_genome), y_pre.ravel(), um_n_params)[0]\n",
    "    # PDE uncertainty\n",
    "    pde_uncertainty = np.linalg.norm(np.sqrt(np.diag(um.sigma_)), 1)/np.linalg.norm(um.coef_, 1)\n",
    "\n",
    "    pde_stat = (ssr, pde_uncertainty)\n",
    "    if um_n_params not in F or F[um_n_params] < pde_stat:\n",
    "        F[um_n_params] = pde_stat\n",
    "\n",
    "del numerical_genome\n",
    "assert len(pareto_optimal_models) > 2\n",
    "\n",
    "F = np.column_stack((list(F.keys()), list(F.values())))\n",
    "F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f259f587",
   "metadata": {},
   "source": [
    "### MCDM/MCDA ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdf1495e-4126-4fc4-9fec-0a71ad6cb539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [0.64356593 0.35643407]\n",
      "[[ 1.          0.19443201]\n",
      " [ 2.          0.06720307]\n",
      " [ 3.          0.06608183]\n",
      " [ 4.          0.06530258]\n",
      " [ 7.          0.06077797]\n",
      " [ 8.          0.06082568]\n",
      " [11.          0.05643292]\n",
      " [13.          0.0561608 ]] [(1, 4)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from pymcdm import weights as obj_w\n",
    "from compromise_programming import mcdm\n",
    "from bayesian_model_evidence import log_evidence\n",
    "\n",
    "include_uncertainty = False\n",
    "use_information_criterion = False\n",
    "\n",
    "# Pseudocode: ใช้ F ได้เลยไม่ต้อง nF\n",
    "nF = F.copy()\n",
    "nF[:, -1] = nF[:, -1] / nF[:, -1].min()\n",
    "if use_information_criterion:\n",
    "    nF[:, -2] = nF[:, -2] - nF[:, -2].min() # relative ic\n",
    "if not include_uncertainty:\n",
    "    nF = nF[:, :-1]\n",
    "\n",
    "types = np.array([-1 for _ in range(nF.shape[-1])])\n",
    "# mcdm weights\n",
    "obj_weights = obj_w.gini_weights(nF, types=types)\n",
    "print(\"Weights:\", obj_weights)\n",
    "# recursive mcdm\n",
    "filtered_F = nF.copy()\n",
    "while len(filtered_F) > 2:\n",
    "    ranks, prefs = mcdm(filtered_F, obj_weights, types)\n",
    "    most_common = Counter(np.argmin(ranks, axis=1)).most_common()\n",
    "    most_common = sorted(most_common, key=lambda _: (_[1], _[0]), reverse=True)\n",
    "    print(filtered_F, most_common)\n",
    "\n",
    "    # keep_until = max(most_common, key=lambda _: _[0])[0]\n",
    "    keep_until = most_common[0][0]\n",
    "    filtered_F = filtered_F[:keep_until+1]\n",
    "    if len(most_common) == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdf3bf3-e292-465a-812a-352906083ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67083870-51a1-42cd-bca3-03a0065d7cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbbfc703",
   "metadata": {},
   "source": [
    "### Evaluate PDE coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7d1ef29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  2.93it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        , -0.33694349,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.0929737 ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        , -0.9744135 ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ]]),\n",
       " [(7,), (8, 13)])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brute_force_all_subsets(X_pre, y_pre, max_support_size=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sindy]",
   "language": "python",
   "name": "conda-env-sindy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
