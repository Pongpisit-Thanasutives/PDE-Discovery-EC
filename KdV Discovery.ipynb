{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8401bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mrmr is not installed in the env you are using. This may cause an error in future if you try to use the (missing) lib.\n",
      "L0BnB is not installed.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from functools import partial\n",
    "from decimal import Decimal\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import pysindy as ps\n",
    "from tqdm import trange\n",
    "\n",
    "from pymoo_ga import *\n",
    "# NSGA2, DNSGA2, SMSEMOA, AGEMOEA2\n",
    "from pymoo.algorithms.moo.nsga2 import NSGA2\n",
    "from pymoo.algorithms.moo.dnsga2 import DNSGA2\n",
    "from pymoo.algorithms.moo.sms import SMSEMOA\n",
    "from pymoo.algorithms.moo.age2 import AGEMOEA2\n",
    "from pymoo.termination.default import DefaultMultiObjectiveTermination\n",
    "from pymoo.optimize import minimize\n",
    "\n",
    "from utils import *\n",
    "from skimage.restoration import estimate_sigma\n",
    "import bm3d\n",
    "# from okridge.solvel0 import *\n",
    "from solvel0 import solvel0, MIOSR\n",
    "from best_subset import backward_refinement, brute_force, brute_force_all_subsets\n",
    "from UBIC import *\n",
    "from kneed import KneeLocator\n",
    "from bayesian_model_evidence import log_evidence\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import covariance\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.linear_model import BayesianRidge, ARDRegression, lars_path\n",
    "from bayesian_linear_regression import BayesianLinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5916f2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_poly = 6\n",
    "n_derivatives = 6\n",
    "n_modules = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eaa96d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['KdV_sine_rep_big.mat', 'kuramoto_sivishinky.mat', 'lorenz100.npy', 'Wave_equation', 'KdV_rudy.mat', 'lorenz10.npy', 'KG_Exp.mat', 'burgers.mat']\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../PDE-Discovery-EC/Datasets/\"\n",
    "print(os.listdir(data_path))\n",
    "data = sio.loadmat(os.path.join(data_path, \"KdV_rudy.mat\"))\n",
    "u_clean = (data['usol']).real; u = u_clean.copy()\n",
    "x = data['x'].ravel()\n",
    "t = data['t'].ravel()\n",
    "dt = t[1]-t[0]; dx = x[2]-x[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1e0adc",
   "metadata": {},
   "source": [
    "### Add noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "888ee41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise level: 50.0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "noise_type = \"gaussian\"\n",
    "noise_lv = float(50)\n",
    "print(\"Noise level:\", noise_lv)\n",
    "noise = 0.01*np.abs(noise_lv)*(u.std())*np.random.randn(u.shape[0],u.shape[1])\n",
    "u = u + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c890b0",
   "metadata": {},
   "source": [
    "### Denoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9754901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_denoised_data = True\n",
    "# if load_denoised_data:\n",
    "#     print(\"Loading denoised data...\")\n",
    "#     u = np.load(f\"./Denoised_data/kdv_{noise_type}{int(noise_lv)}_bm3d.npy\")\n",
    "# else:\n",
    "#     print(\"denoising...\")\n",
    "#     n_sampled_t = 30\n",
    "    \n",
    "#     kernel = RBF(length_scale=1, length_scale_bounds=(1e-2, 1e3)) + \\\n",
    "#             WhiteKernel(noise_level=1, noise_level_bounds=(1e-10, 1e10))\n",
    "    \n",
    "#     xx = colvec(x)\n",
    "#     u_std = np.ones((u.shape[0], len(t)))\n",
    "#     for i in trange(len(t)):    \n",
    "#         gpr = GaussianProcessRegressor(kernel=kernel, alpha=0.0, \n",
    "#                                        n_restarts_optimizer=10 # 20\n",
    "#                                       )\n",
    "    \n",
    "#         gpr.fit(xx, u[:, i])\n",
    "#         _, ustd = gpr.predict(xx, return_std=True)\n",
    "#         u_std[:, i] = ustd\n",
    "\n",
    "#     est_sigma = u_std.mean() # max also works well\n",
    "#     # est_sigma = (est_sigma+estimate_sigma(u))/2\n",
    "#     u = bm3d.bm3d(u, sigma_psd=est_sigma, \n",
    "#                   stage_arg=bm3d.BM3DStages.ALL_STAGES, \n",
    "#                   blockmatches=(False, False))\n",
    "\n",
    "#     np.save(f\"./Denoised_data/kdv_{noise_type}{int(noise_lv)}_bm3d.npy\", u)\n",
    "\n",
    "np.random.seed(0)\n",
    "fake_noise = np.random.normal(loc=0.0, scale=estimate_sigma(u), size=u.shape)\n",
    "sigmas = estimate_sigma(u+fake_noise)*np.arange(0.1, 2., 0.1)\n",
    "est_sigma = sigmas[np.argmin([((u-bm3d.bm3d(u+fake_noise, sigma_psd=sigma, stage_arg=bm3d.BM3DStages.ALL_STAGES, blockmatches=(False, False)))**2).mean() \\\n",
    "                              for sigma in sigmas])]\n",
    "u = bm3d.bm3d(u, sigma_psd=est_sigma, \n",
    "                  stage_arg=bm3d.BM3DStages.ALL_STAGES, \n",
    "                  blockmatches=(False, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94a61f6f-66b5-491f-9096-8fe47f0ef8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = np.array([x.reshape(-1, 1), t.reshape(1, -1)], dtype=object)\n",
    "X, T = np.meshgrid(x, t)\n",
    "XT = np.asarray([X, T]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aaf2666",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_library = ps.PolynomialLibrary(degree=n_poly, include_bias=False)\n",
    "\n",
    "weak_lib = ps.WeakPDELibrary(\n",
    "    function_library=function_library,\n",
    "    derivative_order=n_derivatives,\n",
    "    p=n_derivatives,\n",
    "    spatiotemporal_grid=XT,\n",
    "    include_bias=True,\n",
    "    K=10000, # 2000, 5000, 10000\n",
    "    diff_kwargs={\"is_uniform\":True}\n",
    ")\n",
    "\n",
    "X_pre = np.array(weak_lib.fit_transform(np.expand_dims(u, -1)))\n",
    "y_pre = weak_lib.convert_u_dot_integral(np.expand_dims(u, -1))\n",
    "feature_names = np.array(weak_lib.get_feature_names())\n",
    "\n",
    "# R_path = \"./Cache/\"\n",
    "# np.save(os.path.join(R_path, f\"X_pre_kdv_noise{int(noise_lv)}.npy\"), X_pre)\n",
    "# np.save(os.path.join(R_path, f\"y_pre_kdv_noise{int(noise_lv)}.npy\"), y_pre)\n",
    "# np.save(os.path.join(R_path, f\"feature_names_kdv.npy\"), feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ad9a594",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_poly = np.array([[p, 0] for p in range(1, n_poly+1)])\n",
    "base_derivative = np.array([[0, d] for d in range(1, n_derivatives+1)])\n",
    "modules = [(0, 0)] if weak_lib.include_bias else []\n",
    "modules += [(p, 0) for p in range(1, n_poly+1)] + \\\n",
    "            [(0, d) for d in range(1, n_derivatives+1)] + \\\n",
    "            [tuple(p+d) for d in base_derivative for p in base_poly]\n",
    "assert len(modules) == len(weak_lib.get_feature_names())\n",
    "base_features = dict(zip(modules, X_pre.T))\n",
    "u_t = y_pre.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb73e146",
   "metadata": {},
   "source": [
    "### Genetic algorithm with NSGA-II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "263f3258",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_size = 500\n",
    "problem = PdeDiscoveryProblem(n_poly, n_derivatives, n_modules, \n",
    "                              base_features, u_t, order_complexity=False, ridge_lambda=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "296e4b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_pareto_front = True\n",
    "\n",
    "if not load_pareto_front:\n",
    "    termination = DefaultMultiObjectiveTermination(\n",
    "        xtol=1e-8,\n",
    "        cvtol=1e-6,\n",
    "        ftol=1e-8,\n",
    "        period=50,\n",
    "        n_max_gen=100,\n",
    "        n_max_evals=100000\n",
    "    )\n",
    "\n",
    "    from pymoo.algorithms.moo.sms import SMSEMOA\n",
    "\n",
    "    # algorithm = NSGA2(\n",
    "    #                 pop_size=pop_size, \n",
    "    #                 sampling=PopulationSampling(), \n",
    "    #                 crossover=GenomeCrossover(), \n",
    "    #                 mutation=GenomeMutation(), \n",
    "    #                 eliminate_duplicates=DuplicateElimination()\n",
    "    #                 )\n",
    "\n",
    "    # algorithm = DNSGA2(\n",
    "    #                 pop_size=pop_size,\n",
    "    #                 sampling=PopulationSampling(),\n",
    "    #                 crossover=GenomeCrossover(),\n",
    "    #                 mutation=GenomeMutation(),\n",
    "    #                 eliminate_duplicates=DuplicateElimination()\n",
    "    #                 )\n",
    "\n",
    "    algorithm = SMSEMOA(\n",
    "                    pop_size=pop_size,\n",
    "                    sampling=PopulationSampling(),\n",
    "                    crossover=GenomeCrossover(),\n",
    "                    mutation=GenomeMutation(),\n",
    "                    eliminate_duplicates=DuplicateElimination()\n",
    "                    )\n",
    "\n",
    "    res = minimize(problem, \n",
    "                   algorithm, \n",
    "                   termination=termination, \n",
    "                   verbose=True\n",
    "                  )\n",
    "    \n",
    "    pareto_optimal_models = res.X\n",
    "    np.save(f\"./Cache/pf_SMSEMOA_kdv_noise{int(noise_lv)}.npy\", pareto_optimal_models)\n",
    "\n",
    "else:\n",
    "    pareto_optimal_models = np.load(f\"./Cache/pf_SMSEMOA_kdv_noise{int(noise_lv)}.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e54ed14-bcc4-43bf-a345-f02f3d4da828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[frozenset({(0, 1)})],\n",
       "       [frozenset({(1, 1), (0, 3)})],\n",
       "       [frozenset({(0, 1), (1, 1), (0, 3)})],\n",
       "       [frozenset({(1, 1), (0, 3), (1, 3), (0, 5)})],\n",
       "       [frozenset({(0, 1), (2, 1), (0, 3), (5, 1), (0, 5)})],\n",
       "       [frozenset({(0, 1), (2, 1), (0, 3), (5, 1), (0, 5), (1, 3)})],\n",
       "       [frozenset({(0, 1), (2, 1), (0, 3), (5, 1), (2, 3), (0, 5), (1, 3)})],\n",
       "       [frozenset({(0, 1), (2, 1), (0, 3), (5, 1), (2, 3), (3, 3), (0, 5), (1, 3)})],\n",
       "       [frozenset({(0, 1), (2, 1), (0, 3), (5, 1), (2, 3), (3, 3), (0, 5), (6, 3), (1, 3)})],\n",
       "       [frozenset({(0, 1), (2, 1), (0, 3), (5, 1), (2, 3), (3, 3), (0, 5), (6, 6), (6, 3), (1, 3)})],\n",
       "       [frozenset({(0, 1), (4, 0), (2, 1), (0, 3), (5, 1), (2, 3), (3, 3), (0, 5), (6, 6), (6, 3), (1, 3)})],\n",
       "       [frozenset({(0, 1), (4, 0), (2, 1), (6, 5), (0, 3), (5, 1), (2, 3), (3, 3), (0, 5), (6, 6), (6, 3), (1, 3)})],\n",
       "       [frozenset({(0, 1), (4, 0), (2, 1), (6, 5), (0, 3), (5, 1), (2, 3), (4, 5), (3, 3), (0, 5), (6, 6), (6, 3), (1, 3)})],\n",
       "       [frozenset({(0, 1), (4, 0), (2, 1), (6, 5), (4, 1), (0, 3), (5, 1), (2, 3), (4, 5), (3, 3), (0, 5), (6, 6), (6, 3), (1, 3)})]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### OPTIONAL ###\n",
    "from operator import itemgetter\n",
    "\n",
    "effective_candidates = extract_unique_candidates(pareto_optimal_models)\n",
    "\n",
    "new_pareto_optimal_models = []\n",
    "for bs in backward_refinement([sorted([effective_candidates.index(_) for _ in list(pm[0])]) for pm in pareto_optimal_models], \n",
    "                              (problem.numericalize_genome(effective_candidates), y_pre)).get_best_subsets():\n",
    "    bs = itemgetter(*bs)(effective_candidates)\n",
    "    if type(bs[0]) is not tuple:\n",
    "        bs = (bs,)\n",
    "    new_pareto_optimal_models.append([frozenset(bs)])\n",
    "pareto_optimal_models = np.array(new_pareto_optimal_models)\n",
    "del new_pareto_optimal_models\n",
    "pareto_optimal_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f3d598-a23e-4d55-9add-cabe1114c4a2",
   "metadata": {},
   "source": [
    "### Top candidates by SHAP or Lasso/Lars path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e083fb2b-8c63-4729-a8b0-c15c6e747924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1],\n",
       " [0, 3],\n",
       " [1, 1],\n",
       " [0, 4],\n",
       " [6, 6],\n",
       " [0, 5],\n",
       " [4, 0],\n",
       " [1, 3],\n",
       " [2, 1],\n",
       " [5, 1],\n",
       " [6, 5],\n",
       " [2, 3],\n",
       " [6, 3],\n",
       " [4, 2]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature_importance = dict(zip(effective_candidates, [0.0 for _ in range(len(effective_candidates))]))\n",
    "\n",
    "# for bs in pareto_optimal_models[1:]:\n",
    "#     bs = list(bs[0])\n",
    "#     shap_importance = shap_linear_importance(problem.numericalize_genome(bs), y_pre, scale=False)\n",
    "#     for i, _ in enumerate(bs):\n",
    "#         feature_importance[_] += shap_importance[i]\n",
    "\n",
    "# top_candidates = sorted([(v, k) for k, v in feature_importance.items()], reverse=True)\n",
    "# top_candidates = [v for k, v in top_candidates[:16]]\n",
    "\n",
    "_, lars_p, _ = lars_path(StandardScaler().fit_transform(problem.numericalize_genome(effective_candidates)), \n",
    "                         y_pre.ravel(), method='lasso', alpha_min=1e-5)\n",
    "top_candidates = np.array(effective_candidates)[lars_p].tolist()\n",
    "\n",
    "top_candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3db842-5015-4cee-9a3e-fc65c8f4742e",
   "metadata": {},
   "source": [
    "### Best-subset selections (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4786f593-b73a-4134-a3dc-6113d7c647ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_pre_top = problem.numericalize_genome(top_candidates)\n",
    "# X_pre_top, X_pre_top_norm = normalize_lp(X_pre_top, p=2, axis=0)\n",
    "\n",
    "# best_subsets = solvel0(X_pre_top, y_pre, miosr=True, refine=True)\n",
    "# pareto_optimal_models = [[np.array(top_candidates)[list(bs)]] for bs in best_subsets]\n",
    "\n",
    "# _, _, pde_uncertainties = baye_uncertainties(best_subsets, (X_pre_top, y_pre), \n",
    "#                                              u_type='cv1', take_sqrt=True, \n",
    "#                                              ridge_lambda=0, \n",
    "#                                              threshold=0)\n",
    "\n",
    "# best_subsets, pde_uncertainties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5178a2a-e9c1-4b62-988e-73d1dae23e82",
   "metadata": {},
   "source": [
    "### Uncertainty quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84fd3cc4-20cf-4a62-b70f-86537ac149f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 1.50358996e+00, 2.29351010e-03],\n",
       "       [2.00000000e+00, 4.91956095e-01, 2.25109788e-03],\n",
       "       [3.00000000e+00, 4.28116439e-01, 7.21473237e-03],\n",
       "       [4.00000000e+00, 3.53383350e-01, 5.90528193e-03],\n",
       "       [5.00000000e+00, 3.47036961e-01, 1.46589398e-02],\n",
       "       [6.00000000e+00, 3.30501413e-01, 1.85889972e-02],\n",
       "       [7.00000000e+00, 3.20173507e-01, 1.84666373e-02],\n",
       "       [8.00000000e+00, 3.12971435e-01, 2.14075350e-02],\n",
       "       [9.00000000e+00, 3.11206631e-01, 3.62476325e-02],\n",
       "       [1.10000000e+01, 3.09663329e-01, 3.82290058e-02],\n",
       "       [1.20000000e+01, 3.09643271e-01, 3.83328409e-02],\n",
       "       [1.40000000e+01, 3.08433342e-01, 1.15373287e-01]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numericalize_genome = False\n",
    "F = {}\n",
    "\n",
    "# for each number of active terms -> keep the best coef (in terms of ssr) and track its uncertainty...\n",
    "for bs in pareto_optimal_models:\n",
    "    numerical_genome = problem.numericalize_genome(bs[0])\n",
    "    if numericalize_genome:\n",
    "        numerical_genome = normalize_lp(numerical_genome)[0]\n",
    "    \n",
    "    # um = BayesianLinearRegression() # seems to work well with numericalize_genome = True\n",
    "    um = ARDRegression(fit_intercept=False, compute_score=True, max_iter=1000)\n",
    "    um.fit(numerical_genome, y_pre.ravel())\n",
    "    \n",
    "    # number of effective parameters\n",
    "    um_n_params = np.count_nonzero(um.coef_)\n",
    "    # SSR\n",
    "    ssr = np.sum((um.predict(numerical_genome) - y_pre.ravel())**2)\n",
    "    # IC\n",
    "    bic = BIC_AIC(um.predict(numerical_genome), y_pre.ravel(), um_n_params)[0]\n",
    "    # PDE uncertainty\n",
    "    pde_uncertainty = np.linalg.norm(np.sqrt(np.diag(um.sigma_)), 1)/np.linalg.norm(um.coef_, 1)\n",
    "\n",
    "    pde_stat = (ssr, pde_uncertainty)\n",
    "    if um_n_params not in F or F[um_n_params] < pde_stat:\n",
    "        F[um_n_params] = pde_stat\n",
    "\n",
    "del numerical_genome\n",
    "assert len(pareto_optimal_models) > 2\n",
    "\n",
    "F = np.column_stack((list(F.keys()), list(F.values())))\n",
    "F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437ee0ba",
   "metadata": {},
   "source": [
    "### MCDM/MCDA ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8a1ff62-c3fa-419e-90da-defbbe8077ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [0.56903438 0.43096562]\n",
      "[[ 1.          1.50358996]\n",
      " [ 2.          0.49195609]\n",
      " [ 3.          0.42811644]\n",
      " [ 4.          0.35338335]\n",
      " [ 5.          0.34703696]\n",
      " [ 6.          0.33050141]\n",
      " [ 7.          0.32017351]\n",
      " [ 8.          0.31297144]\n",
      " [ 9.          0.31120663]\n",
      " [11.          0.30966333]\n",
      " [12.          0.30964327]\n",
      " [14.          0.30843334]] [(1, 4)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from pymcdm import weights as obj_w\n",
    "from compromise_programming import mcdm\n",
    "from bayesian_model_evidence import log_evidence\n",
    "\n",
    "include_uncertainty = False\n",
    "use_information_criterion = False\n",
    "\n",
    "# Pseudocode: ใช้ F ได้เลยไม่ต้อง nF\n",
    "nF = F.copy()\n",
    "nF[:, -1] = nF[:, -1] / nF[:, -1].min()\n",
    "if use_information_criterion:\n",
    "    nF[:, -2] = nF[:, -2] - nF[:, -2].min()\n",
    "if not include_uncertainty:\n",
    "    nF = nF[:, :-1]\n",
    "\n",
    "types = np.array([-1 for _ in range(nF.shape[-1])])\n",
    "# mcdm weights\n",
    "obj_weights = obj_w.gini_weights(nF, types=types)\n",
    "print(\"Weights:\", obj_weights)\n",
    "# recursive mcdm\n",
    "filtered_F = nF.copy()\n",
    "while len(filtered_F) > 2:\n",
    "    ranks, prefs = mcdm(filtered_F, obj_weights, types)\n",
    "    most_common = Counter(np.argmin(ranks, axis=1)).most_common()\n",
    "    most_common = sorted(most_common, key=lambda _: (_[1], _[0]), reverse=True)\n",
    "    print(filtered_F, most_common)\n",
    "\n",
    "    # keep_until = max(most_common, key=lambda _: _[0])[0]\n",
    "    keep_until = most_common[0][0]\n",
    "    filtered_F = filtered_F[:keep_until+1]\n",
    "    if len(most_common) == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0cbd86-5c2a-43e1-b3b0-1a74081566ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c015cf35-ca44-45d1-b0d0-396b8560ef60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "886dc85b-79cc-4f18-8aaa-83b04cc71581",
   "metadata": {},
   "source": [
    "### Intercept or NO Intercept? ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9cf0ff9-a357-4220-bf3c-f009d5daf61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_indices = [9, 13]\n",
    "# true_coefficients = [-1, -6]\n",
    "# true_ols = sm.OLS(y_pre, X_pre[:, true_indices]).fit()\n",
    "# estimated_coefficients = true_ols.params\n",
    "# print(estimated_coefficients, mean_absolute_percentage_error(true_coefficients, estimated_coefficients))\n",
    "# true_ols.bic, sm.OLS(y_pre, X_pre[:, [0] + true_indices]).fit().bic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:econ]",
   "language": "python",
   "name": "conda-env-econ-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
