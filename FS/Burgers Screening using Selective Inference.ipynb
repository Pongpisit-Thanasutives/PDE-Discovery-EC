{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f83239b-bf12-480d-a091-268680d3344a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alibi is not installed in the environment.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from functools import partial\n",
    "from decimal import Decimal\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import pysindy as ps\n",
    "from tqdm import trange\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "from utils import *\n",
    "from solvel0 import solvel0, MIOSR\n",
    "from best_subset import backward_refinement, brute_force_all_subsets\n",
    "from UBIC import *\n",
    "from bayesian_model_evidence import log_evidence\n",
    "\n",
    "from skimage.restoration import estimate_sigma\n",
    "import bm3d\n",
    "from kneed import KneeLocator\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "from sklearn import covariance\n",
    "from sklearn.linear_model import lars_path\n",
    "from abess import LinearRegression as AbessLinearRegression\n",
    "from knockpy import KnockoffFilter, knockoff_stats, knockoffs\n",
    "from knockpy.utilities import estimate_covariance\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from c2st.check import c2st # https://github.com/psteinb/c2st\n",
    "\n",
    "from mbic import mbic, mbic2, ebic\n",
    "\n",
    "from rdata import read_rds\n",
    "from selective_inference import forward_stop_rule, sfs_si, stepwise_selective_inference, subset_fdr\n",
    "import fpsample\n",
    "from dppy.finite_dpps import FiniteDPP\n",
    "\n",
    "from si4pipeline import (\n",
    "                        construct_pipelines, \n",
    "                        extract_features, \n",
    "                        initialize_dataset, \n",
    "                        intersection, \n",
    "                        lasso, \n",
    "                        marginal_screening, \n",
    "                        stepwise_feature_selection, \n",
    "                        union, \n",
    "                        PipelineManager\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a033bb19-b956-4d36-9efc-37371989dd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../Datasets/\"\n",
    "data = sio.loadmat(os.path.join(data_path, \"burgers.mat\"))\n",
    "u_clean = (data['usol']).real; u = u_clean.copy()\n",
    "x = (data['x'][0]).real\n",
    "t = (data['t'][:,0]).real\n",
    "xt = np.array([x, t], dtype=object)\n",
    "dt = t[1]-t[0]; dx = x[2]-x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5de3021a-e840-4dc0-9ebe-c6de79e51135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise level: 50.0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "noise_type = \"gaussian\"\n",
    "noise_lv = float(50)\n",
    "print(\"Noise level:\", noise_lv)\n",
    "noise = 0.01*np.abs(noise_lv)*(u.std())*np.random.randn(u.shape[0],u.shape[1])\n",
    "u = u + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce837b01-058c-4b85-9493-d1f5ec41228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "fake_noise = np.random.normal(loc=0.0, scale=estimate_sigma(u), size=u.shape)\n",
    "sigmas = estimate_sigma(u+fake_noise)*np.arange(0.1, 2., 0.1)\n",
    "est_sigma = sigmas[np.argmin([((u-bm3d.bm3d(u+fake_noise, sigma_psd=sigma, stage_arg=bm3d.BM3DStages.ALL_STAGES, blockmatches=(False, False)))**2).mean() \\\n",
    "                              for sigma in sigmas])]\n",
    "u = bm3d.bm3d(u, sigma_psd=est_sigma, \n",
    "                  stage_arg=bm3d.BM3DStages.ALL_STAGES, \n",
    "                  blockmatches=(False, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96244a07-bc2f-47f3-8919-fd1ad48354a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_poly = 6\n",
    "n_derivatives = 6\n",
    "n_weak = 2000\n",
    "function_library = ps.PolynomialLibrary(degree=n_poly, include_bias=False)\n",
    "\n",
    "weak_lib = ps.WeakPDELibrary(\n",
    "    function_library=function_library,\n",
    "    derivative_order=n_derivatives,\n",
    "    spatiotemporal_grid=np.asarray([*np.meshgrid(x, t)]).T,\n",
    "    include_bias=True,\n",
    "    diff_kwargs={\"is_uniform\":True},\n",
    "    K=n_weak\n",
    ")\n",
    "\n",
    "X_pre = np.array(weak_lib.fit_transform(np.expand_dims(u, -1)))\n",
    "y_pre = weak_lib.convert_u_dot_integral(np.expand_dims(u, -1))\n",
    "feature_names = np.array(weak_lib.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a12674e-6ae5-46a4-84bd-115c6ccf295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cache ###\n",
    "# X_pre = np.load(\"../Cache/X_pre_burgers_noise50.npy\")\n",
    "# y_pre = np.load(\"../Cache/y_pre_burgers_noise50.npy\")\n",
    "# u_pre = y_pre.copy()\n",
    "# feature_names = np.load(\"../Cache/feature_names_burgers.npy\")\n",
    "# fsInf = read_rds(\"../R/R_data/fsInf_screening_burgers_noise50.rds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488ecb7c-506d-4aad-b013-a03aecef8570",
   "metadata": {},
   "source": [
    "### Selective inference ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d696abea-ed31-4254-982c-c66d63979c10",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff654fde-eb7c-4b95-8376-1b524e321670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2026-04-04\n",
      "[13  8 31 10 17 44 25 23 43 36  6 12 48  5 20 21] [0.0, 0.0, 0.0, 0.0, 0.0, 0.06405712341604064, 8.945066909404886e-13, 1.2557997974838031e-09, 0.010776968849477875, 0.0, 0.0, 8.84764189250653e-05, 0.0, 0.0, 1.4925283231548292e-11, 0.0002840211046996055] 0.004838052356998551\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.01,\n",
       " array(['x0^5', 'x0^6', 'x0_11', 'x0_1111', 'x0_111111', 'x0x0_1',\n",
       "        'x0^5x0_1', 'x0^2x0_11', 'x0^3x0_11', 'x0^5x0_11', 'x0x0_111',\n",
       "        'x0x0_1111', 'x0^6x0_1111', 'x0x0_111111', 'x0^2x0_111111',\n",
       "        'x0^6x0_111111'], dtype='<U13'),\n",
       " 16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_terms = 16\n",
    "max_complexity = 12\n",
    "alphas = [0.3, 0.25, 0.2, 0.15, 0.1, 0.05, 0.01]\n",
    "\n",
    "### OPTION-I ###\n",
    "# _, lars_p, _ = lars_path(StandardScaler().fit_transform(X_pre), y_pre.flatten(), method='lasso', alpha_min=1e-6, max_iter=1000)\n",
    "# lars_p = np.array(list(map(int, lars_p)))[:n_terms]\n",
    "\n",
    "### OPTION-II ###\n",
    "nonzero = np.nonzero(AbessLinearRegression(s_min=1, s_max=n_terms, path_type='gs', fit_intercept=False, alpha=1e-9, max_iter=100).fit(X_pre, y_pre.flatten()).coef_)[0]\n",
    "nonzero = np.nonzero(MIOSR(X_pre, y_pre, alpha=1e-9, non_zero=min(len(nonzero), n_terms)))[0]\n",
    "_, lars_p, _ = lars_path(StandardScaler().fit_transform(X_pre[:, nonzero]), y_pre.flatten(), method='lasso', alpha_min=0)\n",
    "lars_p = nonzero[np.array(list(map(int, lars_p)))][:n_terms]\n",
    "\n",
    "X_test = X_pre[:, lars_p]\n",
    "sigma = np.std(y_pre-X_test@np.linalg.lstsq(X_test, y_pre)[0], ddof=1)\n",
    "manager = stepwise_selective_inference(support_size=len(lars_p))\n",
    "_, p_list = manager.inference(X_test, y_pre, sigma)\n",
    "print(lars_p, p_list, subset_fdr(p_list))\n",
    "\n",
    "for alpha in alphas:\n",
    "    adjusted_pvalues = p_list\n",
    "    stop_step, false_discovery_rates = forward_stop_rule(adjusted_pvalues, alpha)\n",
    "    adjusted_pvalues = adjusted_pvalues[:stop_step+1]\n",
    "    rejections = np.sort(lars_p[:stop_step+1])\n",
    "    if len(rejections) <= max_complexity: \n",
    "        break\n",
    "max_fdr = alpha\n",
    "max_fdr, feature_names[rejections], len(rejections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c7486a-a24d-4417-824b-8adb5aeb0b94",
   "metadata": {},
   "source": [
    "### R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93dc47a4-a735-4811-bf69-5d6a21319401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_complexity = 12\n",
    "# alphas = [0.3, 0.2, 0.1, 0.05, 0.01]\n",
    "# for alpha in alphas:\n",
    "#     adjusted_pvalues = fsInf.get(\"pv\")\n",
    "#     stop_step, false_discovery_rates = forward_stop_rule(adjusted_pvalues, alpha)\n",
    "#     adjusted_pvalues = adjusted_pvalues[:stop_step+1]\n",
    "#     rejections = np.sort((fsInf.get(\"vars\")-1).astype(np.int32)[:stop_step+1])\n",
    "#     if len(rejections) <= max_complexity:\n",
    "#         break\n",
    "# max_fdr = alpha\n",
    "# feature_names[rejections]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1daad6a-3b72-41de-973f-0a7482299969",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pre_top = X_pre[:, rejections]\n",
    "X_pre_top = X_pre_top/np.linalg.norm(X_pre_top, 2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af29d2b9-fd4a-45c9-a8bf-b02dcbf5a13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 8/8 [00:10<00:00,  1.30s/it]\n"
     ]
    }
   ],
   "source": [
    "_, best_subsets = brute_force_all_subsets(X_pre_top, y_pre, max_support_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89342b11-612e-4129-9fba-cedcb268fdc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assume that mbics is a decreasing sequence\n",
    "complexities = np.array([len(_) for _ in best_subsets])\n",
    "\n",
    "if len(best_subsets) <= 2:\n",
    "    knee = complexities.max()\n",
    "else:\n",
    "    ebics = []\n",
    "    mbics = []\n",
    "    for _ in best_subsets:\n",
    "        loglik = log_like_value(X_pre_top[:, _]@np.linalg.lstsq(X_pre_top[:, _], y_pre, rcond=None)[0], \n",
    "                                y_pre)\n",
    "        ebics.append(ebic(loglik, len(_), len(y_pre), X_pre_top.shape[-1], const=0))\n",
    "        mbics.append(mbic(loglik, len(_), len(y_pre), X_pre_top.shape[-1], const=2))\n",
    "    ebics = np.array(ebics)\n",
    "    mbics = np.array(mbics)\n",
    "\n",
    "    if np.alltrue(np.array(mbics) >= np.array([max(mbics)+_*(min(mbics)-max(mbics))/(np.argmin(mbics)-np.argmax(mbics)) for _ in range(len(best_subsets))])):\n",
    "        knee = complexities.max()\n",
    "    else:    \n",
    "        decreasing_indices = np.array(mbics) <= np.array([max(mbics)+_*(min(mbics)-max(mbics))/(np.argmin(mbics)-np.argmax(mbics)) for _ in range(len(best_subsets))])\n",
    "        knee = knee_finder(mbics[decreasing_indices])\n",
    "        knee = (complexities[decreasing_indices])[knee]\n",
    "    \n",
    "knee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e27c59e-f740-4e66-92b7-e927a743b26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max fdr: 0.01\n",
      "1 0.0 0.0625\n",
      "2 5.963071703582124e-11 0.0625\n",
      "3 0.04096051873249207 1.0\n",
      "4 0.0915303306141322 1.0\n",
      "5 0.3244726513182708 1.0\n",
      "6 0.4194391611415963 1.0\n",
      "7 0.3874262721222558 1.0\n",
      "8 0.519674554645266 1.0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0); random.seed(0)\n",
    "n_samples = min(int(250*knee), len(y_pre))\n",
    "false_discovery_control_method = 'bh'\n",
    "print(\"max fdr:\", max_fdr)\n",
    "fdr_data = []\n",
    "for bs in best_subsets:\n",
    "    fdrs = []\n",
    "    for _ in range(len(y_pre)//n_samples):\n",
    "        X_test = X_pre_top[:, bs]\n",
    "        y_test = y_pre.ravel()\n",
    "        \n",
    "        np.random.seed(random.randint(0, 100))\n",
    "        # sample_indices = sorted(set([np.random.randint(len(y_pre)) for _ in range(n_samples)]))\n",
    "        sample_indices = fpsample.bucket_fps_kdline_sampling(X_test, n_samples=n_samples, h=3) # Farthest Point Sampling (FPS) is better!!!\n",
    "        X_test = X_test[sample_indices]; y_test = y_test[sample_indices]\n",
    "        # FPS + k-DPP\n",
    "        DPP = FiniteDPP('likelihood', **{'L': X_test.dot(X_test.T)})\n",
    "        DPP.flush_samples()\n",
    "        for _ in range(n_samples//(len(bs))):\n",
    "            DPP.sample_exact_k_dpp(size=len(bs))\n",
    "        sample_indices = np.unique(np.ravel(DPP.list_of_samples))\n",
    "        X_test = X_test[sample_indices]; y_test = y_test[sample_indices]\n",
    "        \n",
    "        manager = stepwise_selective_inference(support_size=X_test.shape[1])\n",
    "        M, p_list = manager.inference(X_test, y_test, np.std(y_test))\n",
    "        if false_discovery_control_method is not None:\n",
    "            p_list = stats.false_discovery_control(p_list, method=false_discovery_control_method)\n",
    "        # print(M, p_list, np.array(p_list) < 0.05)\n",
    "        fdrs.append(subset_fdr(p_list))\n",
    "        \n",
    "    fdrs = np.array(fdrs)\n",
    "    if fdrs.mean() < 1:\n",
    "        print(len(bs), fdrs.mean(), stats.wilcoxon(fdrs-max_fdr, alternative='less').pvalue)\n",
    "        fdr_data.append(fdrs)\n",
    "        \n",
    "fdr_data = np.array(fdr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77d997f5-ddf0-4c75-98ab-174feaa468d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 1 1 1]\n",
      "[0 0 0 0 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AffinityPropagation, KMeans\n",
    "print(AffinityPropagation().fit(fdr_data).labels_)\n",
    "print(KMeans(n_clusters=2).fit(fdr_data).labels_)\n",
    "# plt.plot([1, 2, 3, 4], fdr_data.mean(axis=-1), 'o'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a7e323b-798f-4e98-9a8f-659083e7fca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max fdr: 0.01\n",
      "0.0 0.0625\n",
      "0.0 0.0625\n",
      "0.00706043037469649 0.0625\n",
      "0.02662044933457 1.0\n",
      "0.17572923806322494 1.0\n",
      "0.18509861285699977 1.0\n",
      "0.17351220136329723 1.0\n",
      "0.21174723884084515 1.0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0); random.seed(0)\n",
    "n_samples = min(int(250*knee), len(X_pre))\n",
    "false_discovery_control_method = 'bh'\n",
    "print(\"max fdr:\", max_fdr)\n",
    "for bs in best_subsets:\n",
    "    fdrs = []\n",
    "    for _ in range(len(y_pre)//n_samples):\n",
    "        X_test = X_pre_top[:, bs]\n",
    "        y_test = y_pre.ravel()\n",
    "        \n",
    "        np.random.seed(random.randint(0, 100))\n",
    "        # sample_indices = sorted(set([np.random.randint(len(y_pre)) for _ in range(n_samples)]))\n",
    "        sample_indices = fpsample.bucket_fps_kdline_sampling(X_test, n_samples=n_samples, h=3) # Farthest Point Sampling (FPS) is better!!!\n",
    "        X_test = X_test[sample_indices]; y_test = y_test[sample_indices]\n",
    "        # FPS + k-DPP\n",
    "        # DPP = FiniteDPP('likelihood', **{'L': X_test.dot(X_test.T)})\n",
    "        # DPP.flush_samples()\n",
    "        # for _ in range(n_samples//(len(bs))):\n",
    "        #     DPP.sample_exact_k_dpp(size=len(bs))\n",
    "        # sample_indices = np.unique(np.ravel(DPP.list_of_samples))\n",
    "        # X_test = X_test[sample_indices]; y_test = y_test[sample_indices]\n",
    "        \n",
    "        manager = stepwise_selective_inference(support_size=X_test.shape[1])\n",
    "        M, p_list = manager.inference(X_test, y_test, np.std(y_test))\n",
    "        if false_discovery_control_method is not None:\n",
    "            p_list = stats.false_discovery_control(p_list, method=false_discovery_control_method)\n",
    "        # print(M, p_list, np.array(p_list) < 0.05)\n",
    "        fdrs.append(subset_fdr(p_list))\n",
    "        \n",
    "    fdrs = np.array(fdrs)\n",
    "    print(fdrs.mean(), stats.wilcoxon(fdrs-max_fdr, alternative='less').pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d199725d-a37e-4939-ba3e-45a9370e27e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bf1b8d-f7f2-4d83-94ca-731ec9f4cfd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sindy]",
   "language": "python",
   "name": "conda-env-sindy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
