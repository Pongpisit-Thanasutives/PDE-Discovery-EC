{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f83239b-bf12-480d-a091-268680d3344a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alibi is not installed in the environment.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from functools import partial\n",
    "from decimal import Decimal\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import pysindy as ps\n",
    "from tqdm import trange\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "from utils import *\n",
    "from solvel0 import solvel0, MIOSR\n",
    "from best_subset import backward_refinement, brute_force_all_subsets, brute_force\n",
    "from UBIC import *\n",
    "from bayesian_model_evidence import log_evidence\n",
    "\n",
    "from skimage.restoration import estimate_sigma\n",
    "import bm3d\n",
    "from kneed import KneeLocator\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "from sklearn import covariance\n",
    "from sklearn.linear_model import lars_path\n",
    "from abess import LinearRegression as AbessLinearRegression\n",
    "from knockpy import KnockoffFilter, knockoff_stats, knockoffs\n",
    "from knockpy.utilities import estimate_covariance\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from c2st.check import c2st # https://github.com/psteinb/c2st\n",
    "\n",
    "from mbic import mbic, mbic2, ebic\n",
    "\n",
    "from rdata import read_rds\n",
    "from selective_inference import forward_stop_rule, sfs_si, stepwise_selective_inference, subset_fdr\n",
    "import fpsample\n",
    "from dppy.finite_dpps import FiniteDPP\n",
    "\n",
    "from si4pipeline import (\n",
    "                        construct_pipelines, \n",
    "                        extract_features, \n",
    "                        initialize_dataset, \n",
    "                        intersection, \n",
    "                        lasso, \n",
    "                        marginal_screening, \n",
    "                        stepwise_feature_selection, \n",
    "                        union, \n",
    "                        PipelineManager\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a033bb19-b956-4d36-9efc-37371989dd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../Datasets/\"\n",
    "data = sio.loadmat(os.path.join(data_path, \"burgers.mat\"))\n",
    "u_clean = (data['usol']).real; u = u_clean.copy()\n",
    "x = (data['x'][0]).real\n",
    "t = (data['t'][:,0]).real\n",
    "xt = np.array([x, t], dtype=object)\n",
    "dt = t[1]-t[0]; dx = x[2]-x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5de3021a-e840-4dc0-9ebe-c6de79e51135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise level: 50.0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "noise_type = \"gaussian\"\n",
    "noise_lv = float(50)\n",
    "print(\"Noise level:\", noise_lv)\n",
    "noise = 0.01*np.abs(noise_lv)*(u.std())*np.random.randn(u.shape[0],u.shape[1])\n",
    "u = u + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce837b01-058c-4b85-9493-d1f5ec41228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "fake_noise = np.random.normal(loc=0.0, scale=estimate_sigma(u), size=u.shape)\n",
    "sigmas = estimate_sigma(u+fake_noise)*np.arange(0.1, 2., 0.1)\n",
    "est_sigma = sigmas[np.argmin([((u-bm3d.bm3d(u+fake_noise, sigma_psd=sigma, stage_arg=bm3d.BM3DStages.ALL_STAGES, blockmatches=(False, False)))**2).mean() \\\n",
    "                              for sigma in sigmas])]\n",
    "u = bm3d.bm3d(u, sigma_psd=est_sigma, \n",
    "                  stage_arg=bm3d.BM3DStages.ALL_STAGES, \n",
    "                  blockmatches=(False, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eaed70-cf10-4a34-adaa-91de6c528d2d",
   "metadata": {},
   "source": [
    "### weakident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53378b3-a6d8-464d-b748-b393740f8f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from weakident_python.weakindent_model import weak_ident\n",
    "\n",
    "# # Ground truth\n",
    "# true_coefficients = np.array([np.array([[2, 1, 0, -0.5], [1, 2, 0, 0.1]])])\n",
    "# # Hyperparameters\n",
    "# config = {'true_coefficients': true_coefficients, \n",
    "#           'max_dx': 6, 'max_poly': 6, 'use_cross_der': False, 'skip_x': 7, 'skip_t': 3, 'tau': 0.05}\n",
    "\n",
    "# # Sparse regression\n",
    "# np.random.seed(99)\n",
    "# X_pre, y_pre, c_pred, dictionary_list, lhs_feature, rhs_feature = weak_ident(np.expand_dims(u, 0), xt, **config)\n",
    "# feature_names = rhs_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9048169b-08dc-437f-9122-8cc17c09e892",
   "metadata": {},
   "source": [
    "### ps.WeakPDELibrary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96244a07-bc2f-47f3-8919-fd1ad48354a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_poly = 6\n",
    "n_derivatives = 6\n",
    "n_weak = 2000\n",
    "\n",
    "### Cache ###\n",
    "# X_pre = np.load(\"../Cache/X_pre_burgers_noise50.npy\")\n",
    "# y_pre = np.load(\"../Cache/y_pre_burgers_noise50.npy\")\n",
    "# u_pre = y_pre.copy()\n",
    "# feature_names = np.load(\"../Cache/feature_names_burgers.npy\")\n",
    "# fsInf = read_rds(\"../R/R_data/fsInf_screening_burgers_noise50.rds\")\n",
    "\n",
    "function_library = ps.PolynomialLibrary(degree=n_poly, include_bias=False)\n",
    "\n",
    "weak_lib = ps.WeakPDELibrary(\n",
    "    function_library=function_library,\n",
    "    derivative_order=n_derivatives,\n",
    "    spatiotemporal_grid=np.asarray([*np.meshgrid(x, t)]).T,\n",
    "    include_bias=True,\n",
    "    diff_kwargs={\"is_uniform\":True},\n",
    "    K=n_weak\n",
    ")\n",
    "\n",
    "X_pre = np.array(weak_lib.fit_transform(np.expand_dims(u, -1)))\n",
    "y_pre = weak_lib.convert_u_dot_integral(np.expand_dims(u, -1))\n",
    "feature_names = np.array(weak_lib.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a414c814-9a7f-44d0-88eb-11317ade1287",
   "metadata": {},
   "source": [
    "### Knockoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6320cf13-a664-4cc7-b329-71be6e7029fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hidimstat import (model_x_knockoff, \n",
    "                    model_x_knockoff_pvalue, \n",
    "                    model_x_knockoff_bootstrap_quantile, \n",
    "                    model_x_knockoff_bootstrap_e_value)\n",
    "\n",
    "# selected, test_scores, threshold, X_tildes = model_x_knockoff(X_pre, \n",
    "#                                                               y_pre, \n",
    "#                                                               centered=True, \n",
    "#                                                               n_bootstraps=25, \n",
    "#                                                               fdr=0.1, \n",
    "#                                                               n_jobs=-1)\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from abess import LinearRegression as AbessLinearRegression\n",
    "from knockpy import KnockoffFilter, knockoff_stats, knockoffs\n",
    "from knockpy.utilities import estimate_covariance\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from c2st.check import c2st # https://github.com/psteinb/c2st\n",
    "\n",
    "u_pre = y_pre.copy()\n",
    "X_pre_top = StandardScaler().fit_transform(X_pre)\n",
    "y_pre = StandardScaler().fit_transform(u_pre)\n",
    "\n",
    "lr = AbessLinearRegression(path_type='gs', s_max=10, fit_intercept=False, cv=5, screening_size=0)\n",
    "\n",
    "kfilter = KnockoffFilter(ksampler='gaussian', fstat=knockoff_stats.ShapStatistic(model=lr), \n",
    "                         knockoff_kwargs={'method':'ci'})\n",
    "# kfilter = KnockoffFilter(ksampler='gaussian', fstat=knockoff_stats.FeatureStatistic(model=lr), \n",
    "#                          knockoff_kwargs={'method':'ci'}, fstat_kwargs={'feature_importance':'swapint'})\n",
    "# kfilter = KnockoffFilter(ksampler='gaussian', fstat='lasso', knockoff_kwargs={'method':'ci'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f17a8cd-0ed8-4379-8819-5038573c6105",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdr = 1/3 # 1/4, 1/3, 1/2\n",
    "rejections = []\n",
    "test_scores = []\n",
    "thresholds = []\n",
    "for _ in trange(50):\n",
    "    np.random.seed(_)\n",
    "    # X_resample, y_resample = resample(X_pre_top, y_pre.flatten(), replace=True, random_state=_)\n",
    "    rejection = kfilter.forward(X=X_pre_top, y=y_pre.flatten(), fdr=fdr, shrinkage=\"ledoitwolf\", recycle_up_to=0.5)\n",
    "    rejection = sorted(set(np.where(rejection == 1)[0]))\n",
    "    if len(rejection) > 0:\n",
    "        rejections.append(rejection)\n",
    "        test_scores.append(kfilter.W)\n",
    "        thresholds.append(kfilter.threshold)\n",
    "# del X_resample, y_resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd03c60-6d12-4ba2-b67f-9c110d888395",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_ko_selection, _, _ = model_x_knockoff_bootstrap_quantile(test_scores, \n",
    "                                                                    fdr=fdr, \n",
    "                                                                    adaptive_aggregation=False)\n",
    "print(feature_names[aggregated_ko_selection])\n",
    "\n",
    "# TODO: Calculate 1-FDR\n",
    "# ShapStatistic: ['x0_11' 'x0_1111' 'x0x0_1' 'x0^6x0_1']\n",
    "# swap: ['x0_11' 'x0_1111' 'x0x0_1' 'x0^6x0_1']\n",
    "# swapint: ['x0_11' 'x0_1111' 'x0x0_1' 'x0^6x0_1']\n",
    "eval_selection = []\n",
    "while True:\n",
    "    eval_selection, _, _ = model_x_knockoff_bootstrap_e_value(test_scores, thresholds, fdr=fdr)\n",
    "    print(feature_names[eval_selection])\n",
    "    if len(eval_selection) > 0:\n",
    "        break\n",
    "    else:\n",
    "        fdr += 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f39aa5-7314-4150-810e-d957e5ef89f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rejections = np.array(eval_selection)\n",
    "X_pre_top = X_pre_top[:, rejections]\n",
    "feature_names[rejections]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00dbcf9-e608-4342-81de-beea1faa8d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "classifer_threshold = 0.5\n",
    "while True:\n",
    "    non_null_indices, shap_values = shap_model_selection(X_pre_top, y_pre)\n",
    "    rejections = rejections[non_null_indices]\n",
    "    X_pre_top = X_pre_top[:, non_null_indices]\n",
    "    print(abs(shap_values).mean(axis=0))\n",
    "    print(feature_names[rejections])\n",
    "\n",
    "    decision = True\n",
    "    Sigma, invSigma = estimate_covariance(X_pre_top, 1e-3, \"graphicallasso\") # graphicallasso, ledoitwolf\n",
    "    for j in range(len(rejections)-1, -1, -1):\n",
    "        classifier_confidences = []\n",
    "        for _ in trange(50):\n",
    "            Xk = knockoffs.GaussianSampler(X_pre_top, Sigma=Sigma, invSigma=invSigma, \n",
    "                                           method='ci').sample_knockoffs()\n",
    "            Xn = X_pre_top.copy()\n",
    "            Xn[:, j] = Xk[:, j]\n",
    "            \n",
    "            swap_explainer = shap.explainers.Linear(linear_model.LinearRegression(fit_intercept=False).fit(Xn, y_pre),\n",
    "                                                    Xn)\n",
    "            swap_shap_values = swap_explainer(Xn).values\n",
    "            \n",
    "            classifier_confidences.append(c2st(shap_values[:, j:j+1], swap_shap_values[:, j:j+1], clf=linear_model.LogisticRegression(fit_intercept=True)))\n",
    "    \n",
    "        classifier_confidences = np.array(classifier_confidences)\n",
    "        pv = stats.wilcoxon(classifier_confidences-classifer_threshold, alternative='greater').pvalue\n",
    "        \n",
    "        print(\"binary classifier's acc:\", classifier_confidences.mean())\n",
    "        print(\"P-value:\", pv)\n",
    "    \n",
    "        if not pv < alpha:\n",
    "            decision = False\n",
    "            break\n",
    "\n",
    "    if not decision:\n",
    "        # non_null_indices = list(solvel0(X_pre_top, y_pre, max_complexity=len(rejections)-1, miosr=True, refine=True)[-1])\n",
    "        non_null_indices = np.nonzero(brute_force(X_pre_top, y_pre, support_size=len(rejections)-1))[0]\n",
    "        rejections = rejections[non_null_indices]\n",
    "        X_pre_top = X_pre_top[:, non_null_indices]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Optional\n",
    "nonzero_miosr = np.array([], dtype=np.int32)\n",
    "for _ in range(len(rejections)):\n",
    "    nonzero_miosr = np.union1d(nonzero_miosr, np.nonzero(brute_force(X_pre, y_pre, len(rejections)-_))[0])\n",
    "rejections = np.intersect1d(rejections, nonzero_miosr)\n",
    "feature_names[rejections]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169efe3c-e462-4bae-9239-baa190ec8050",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pre_top = X_pre[:, rejections]\n",
    "X_pre_top = X_pre_top/np.linalg.norm(X_pre_top, 2, axis=0)\n",
    "y_pre = u_pre.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f40c23-ea38-43ed-a63c-f06125fbb975",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_subsets = brute_force_all_subsets(X_pre_top, y_pre)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d204376c-59c1-40b0-8a69-8a1ed6b18909",
   "metadata": {},
   "source": [
    "### Compromise programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660e4be8-9b8e-4ee8-b048-0d087f72e896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from compromise_programming import compromise_programming\n",
    "compromise_programming(best_subsets, (X_pre_top, y_pre))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cc3141-af69-476f-af50-5fb4da60d387",
   "metadata": {},
   "source": [
    "### UBIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d029f9-2674-40f9-81cd-f679b7dcc13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 3\n",
    "verbose = True\n",
    "# scale = 1 <- generalized UBIC\n",
    "scale = np.log(len(y_pre))\n",
    "per = 75 # 80\n",
    "\n",
    "post_means, b_bics, b_uns = baye_uncertainties(best_subsets, (X_pre_top, y_pre), \n",
    "                                               u_type='cv1', take_sqrt=True, \n",
    "                                               ridge_lambda=0, \n",
    "                                               threshold=0)\n",
    "# b_uns = ard_uns # USE ard_uns INSTEAD\n",
    "predictions = X_pre_top@post_means\n",
    "print(b_bics)\n",
    "print(b_uns)\n",
    "b_bics = np.array(b_bics)\n",
    "max_complexity = len(b_bics)\n",
    "complexities = np.arange(max_complexity)+1\n",
    "d_complexities = complexities[decreasing_values_indices(b_bics)]\n",
    "d_bics = b_bics[decreasing_values_indices(b_bics)]\n",
    "slopes = np.diff(b_bics)/(np.diff(complexities)*b_bics[:-1])\n",
    "try:\n",
    "    thres = np.percentile(np.abs(np.diff(d_bics)/(np.diff(d_complexities)*d_bics[:-1])), per)\n",
    "    # None / Round / Ceil / Floor: Decided by researchers to automate the model selection process\n",
    "    thres = np.round(sci_format(thres)[0])*10**sci_format(thres)[1]\n",
    "except IndexError:\n",
    "    thres = 1/40\n",
    "min_thres = 1/40\n",
    "thres = max(thres, min_thres)\n",
    "print(\"threshold:\", thres)\n",
    "\n",
    "lower_bounds = []\n",
    "for k, efi in enumerate(best_subsets):\n",
    "    # assert len(efi) == np.count_nonzero(post_means[:, k:k+1])\n",
    "    com = len(efi)\n",
    "    lower_bound = 2*np.abs(log_like_value(predictions[:, k:k+1], y_pre))-np.log(len(y_pre))*com\n",
    "    lower_bounds.append(lower_bound)\n",
    "\n",
    "last_lam = np.log10(max(lower_bounds/(b_uns*scale)))\n",
    "print(\"max_lam:\", last_lam)\n",
    "delta = last_lam/tau\n",
    "now_lam = last_lam-delta\n",
    "last_ubic = UBIC(b_bics, b_uns, len(y_pre), hyp=10**last_lam, scale=scale)\n",
    "last_bc = np.argmin(last_ubic)\n",
    "bc_seq = [last_bc]\n",
    "while now_lam >= 0:\n",
    "    now_ubic = UBIC(b_bics, b_uns, len(y_pre), hyp=10**now_lam, scale=scale)\n",
    "    now_bc = np.argmin(now_ubic)\n",
    "    \n",
    "    diff_com = now_bc-last_bc\n",
    "    diff_bic = b_bics[now_bc]-b_bics[last_bc]\n",
    "    imp = np.nan\n",
    "    if diff_com != 0:\n",
    "        imp = abs(diff_bic/(b_bics[last_bc]*diff_com))\n",
    "    \n",
    "    if verbose:\n",
    "        print(min(last_bc, now_bc), '<--->', max(last_bc, now_bc), \n",
    "              np.nan_to_num(imp, nan=np.inf))\n",
    "    \n",
    "    if (diff_com > 0 and (diff_bic > 0 or imp < thres)) or \\\n",
    "        (diff_com < 0 and diff_bic > 0 and imp > thres):\n",
    "        break\n",
    "    \n",
    "    last_lam = now_lam\n",
    "    now_lam = round(last_lam-delta, 8)\n",
    "    last_ubic = now_ubic\n",
    "    last_bc = now_bc\n",
    "    if last_bc not in bc_seq:\n",
    "        bc_seq.append(last_bc)\n",
    "\n",
    "# best_bc = knee(range(len(last_ubic)), last_ubic, 0.95, 'linear', direction='decreasing')\n",
    "best_bc = knee_finder(last_ubic)\n",
    "if best_bc == 0 and last_bc != 0 and b_bics[last_bc] < b_bics[0] and \\\n",
    "                                    abs((b_bics[last_bc]-b_bics[0])/(b_bics[0]*last_bc)) > thres:\n",
    "    best_bc = knee(range(1, len(last_ubic)), last_ubic[1:], 0.95, 'linear')\n",
    "if best_bc is None:\n",
    "    best_bc = knee_finder(last_ubic)\n",
    "    \n",
    "last_lam = round(last_lam, 8)\n",
    "last_lam, last_ubic, last_bc, best_bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96439c3c-98eb-48f0-a8e1-d05f006a8472",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "ax.plot([len(bs) for bs in best_subsets], last_ubic, '-o', c='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a39d54-2d79-4f72-986d-fa499c8dfdbc",
   "metadata": {},
   "source": [
    "### Selective inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9512fb-e0aa-45d3-bebd-4fca2871cf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume that mbics is a decreasing sequence\n",
    "complexities = np.array([len(_) for _ in best_subsets])\n",
    "\n",
    "if len(best_subsets) <= 2:\n",
    "    knee = complexities.max()\n",
    "else:\n",
    "    ebics = []\n",
    "    mbics = []\n",
    "    for _ in best_subsets:\n",
    "        loglik = log_like_value(X_pre_top[:, _]@np.linalg.lstsq(X_pre_top[:, _], y_pre, rcond=None)[0], \n",
    "                                y_pre)\n",
    "        ebics.append(ebic(loglik, len(_), len(y_pre), X_pre_top.shape[-1], const=0))\n",
    "        mbics.append(mbic(loglik, len(_), len(y_pre), X_pre_top.shape[-1], const=2))\n",
    "    ebics = np.array(ebics)\n",
    "    mbics = np.array(mbics)\n",
    "\n",
    "    if np.alltrue(np.array(mbics) >= np.array([max(mbics)+_*(min(mbics)-max(mbics))/(np.argmin(mbics)-np.argmax(mbics)) for _ in range(len(best_subsets))])):\n",
    "        knee = complexities.max()\n",
    "    else:    \n",
    "        decreasing_indices = np.array(mbics) <= np.array([max(mbics)+_*(min(mbics)-max(mbics))/(np.argmin(mbics)-np.argmax(mbics)) for _ in range(len(best_subsets))])\n",
    "        knee = knee_finder(mbics[decreasing_indices])\n",
    "        knee = (complexities[decreasing_indices])[knee]\n",
    "    \n",
    "knee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed059dfc-cbe0-485f-93f9-5a13f91ea50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0); random.seed(0)\n",
    "n_samples = min(int(250*knee), len(y_pre))\n",
    "false_discovery_control_method = None\n",
    "fdr_data = []\n",
    "for bs in best_subsets:\n",
    "    fdrs = []\n",
    "    for _ in range(len(y_pre)//n_samples):\n",
    "        X_test = X_pre_top[:, bs]\n",
    "        y_test = y_pre.ravel()\n",
    "        \n",
    "        np.random.seed(random.randint(0, 100))\n",
    "        # sample_indices = sorted(set([np.random.randint(len(y_pre)) for _ in range(n_samples)]))\n",
    "        sample_indices = fpsample.bucket_fps_kdline_sampling(X_test, n_samples=n_samples, h=3) # Farthest Point Sampling (FPS) is better!!!\n",
    "        X_test = X_test[sample_indices]; y_test = y_test[sample_indices]\n",
    "        # FPS + k-DPP\n",
    "        DPP = FiniteDPP('likelihood', **{'L': X_test.dot(X_test.T)})\n",
    "        DPP.flush_samples()\n",
    "        for _ in range(n_samples//(len(bs))):\n",
    "            DPP.sample_exact_k_dpp(size=len(bs))\n",
    "        sample_indices = np.unique(np.ravel(DPP.list_of_samples))\n",
    "        X_test = X_test[sample_indices]; y_test = y_test[sample_indices]\n",
    "        \n",
    "        manager = stepwise_selective_inference(support_size=X_test.shape[1])\n",
    "        M, p_list = manager.inference(X_test, y_test, np.std(y_test))\n",
    "        if false_discovery_control_method is not None:\n",
    "            p_list = stats.false_discovery_control(p_list, method=false_discovery_control_method)\n",
    "        \n",
    "        fdrs.append(subset_fdr(p_list))\n",
    "        \n",
    "    fdrs = np.array(fdrs)\n",
    "    if fdrs.mean() < 1:\n",
    "        print(len(bs), fdrs.mean())\n",
    "        fdr_data.append(fdrs)\n",
    "        \n",
    "fdr_data = np.array(fdr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb8882b-ebca-4f4c-9e30-8106d9f25d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation, KMeans\n",
    "print(AffinityPropagation().fit(fdr_data).labels_)\n",
    "print(KMeans(n_clusters=2).fit(fdr_data).labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230a739f-a039-4f97-82eb-1a60cb22b2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0); random.seed(0)\n",
    "n_samples = min(int(250*knee), len(y_pre))\n",
    "false_discovery_control_method = 'by'\n",
    "fdr_data = []\n",
    "for bs in best_subsets:\n",
    "    fdrs = []\n",
    "    for _ in range(len(y_pre)//n_samples):\n",
    "        X_test = X_pre_top[:, bs]\n",
    "        y_test = y_pre.ravel()\n",
    "        \n",
    "        np.random.seed(random.randint(0, 100))\n",
    "        # sample_indices = sorted(set([np.random.randint(len(y_pre)) for _ in range(n_samples)]))\n",
    "        sample_indices = fpsample.bucket_fps_kdline_sampling(X_test, n_samples=n_samples, h=3) # Farthest Point Sampling (FPS) is better!!!\n",
    "        X_test = X_test[sample_indices]; y_test = y_test[sample_indices]\n",
    "        # FPS + k-DPP\n",
    "        DPP = FiniteDPP('likelihood', **{'L': X_test.dot(X_test.T)})\n",
    "        DPP.flush_samples()\n",
    "        for _ in range(n_samples//(len(bs))):\n",
    "            DPP.sample_exact_k_dpp(size=len(bs))\n",
    "        sample_indices = np.unique(np.ravel(DPP.list_of_samples))\n",
    "        X_test = X_test[sample_indices]; y_test = y_test[sample_indices]\n",
    "        \n",
    "        manager = stepwise_selective_inference(support_size=X_test.shape[1])\n",
    "        M, p_list = manager.inference(X_test, y_test, np.std(y_test))\n",
    "        if false_discovery_control_method is not None:\n",
    "            p_list = stats.false_discovery_control(p_list, method=false_discovery_control_method)\n",
    "        \n",
    "        fdrs.append(subset_fdr(p_list))\n",
    "        \n",
    "    fdrs = np.array(fdrs)\n",
    "    if fdrs.mean() < 1:\n",
    "        print(len(bs), fdrs.mean())\n",
    "        fdr_data.append(fdrs)\n",
    "        \n",
    "fdr_data = np.array(fdr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09d22f5-e0cd-4c95-bc7c-ad5dacec6ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddba864a-68a4-4d81-8341-ff0b5d857639",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "488ecb7c-506d-4aad-b013-a03aecef8570",
   "metadata": {},
   "source": [
    "### Selective inference (with estimated FDR) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d696abea-ed31-4254-982c-c66d63979c10",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff654fde-eb7c-4b95-8376-1b524e321670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13  8 31 10  0 44 19  9 16 12 48  6 11 35 32 39] [0.0, 0.0, 0.07857371647513556, 2.5975666062549863e-11, 0.0897928287576325, 0.0, 1.6850402052570601e-06, 1.077635758406359e-09, 0.0, 0.16881660353373795, 0.0, 0.1386174966412872, 3.544258979903958e-05, 0.0, 0.0, 0.0] 0.03187963178091451\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.01, array(['x0_11', 'x0x0_1'], dtype='<U13'), 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_terms = 16\n",
    "max_complexity = 10\n",
    "alphas = [0.3, 0.25, 0.2, 0.15, 0.1, 0.05, 0.01]\n",
    "\n",
    "### OPTION-I ###\n",
    "_, lars_p, _ = lars_path(StandardScaler().fit_transform(X_pre), y_pre.flatten(), method='lasso', alpha_min=1e-6, max_iter=1000)\n",
    "lars_p = np.array(list(map(int, lars_p)))[:n_terms]\n",
    "\n",
    "### OPTION-II ###\n",
    "# nonzero = np.nonzero(AbessLinearRegression(s_min=1, s_max=n_terms, path_type='gs', fit_intercept=False, alpha=1e-9, max_iter=100).fit(X_pre, y_pre.flatten()).coef_)[0]\n",
    "# nonzero = np.nonzero(MIOSR(X_pre, y_pre, alpha=1e-9, non_zero=min(len(nonzero), n_terms)))[0]\n",
    "# _, lars_p, _ = lars_path(StandardScaler().fit_transform(X_pre[:, nonzero]), y_pre.flatten(), method='lasso', alpha_min=0)\n",
    "# lars_p = nonzero[np.array(list(map(int, lars_p)))][:n_terms]\n",
    "\n",
    "X_test = X_pre[:, lars_p]\n",
    "sigma = np.std(y_pre-X_test@np.linalg.lstsq(X_test, y_pre)[0], ddof=1)\n",
    "manager = stepwise_selective_inference(support_size=len(lars_p))\n",
    "_, p_list = manager.inference(X_test, y_pre, sigma)\n",
    "print(lars_p, p_list, subset_fdr(p_list))\n",
    "\n",
    "for alpha in alphas:\n",
    "    adjusted_pvalues = p_list\n",
    "    stop_step, false_discovery_rates = forward_stop_rule(adjusted_pvalues, alpha)\n",
    "    adjusted_pvalues = adjusted_pvalues[:stop_step+1]\n",
    "    rejections = np.sort(lars_p[:stop_step+1])\n",
    "    if len(rejections) <= max_complexity: \n",
    "        break\n",
    "max_fdr = alpha\n",
    "max_fdr, feature_names[rejections], len(rejections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c7486a-a24d-4417-824b-8adb5aeb0b94",
   "metadata": {},
   "source": [
    "### R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93dc47a4-a735-4811-bf69-5d6a21319401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_complexity = 10\n",
    "# alphas = [0.3, 0.2, 0.1, 0.05, 0.01]\n",
    "# for alpha in alphas:\n",
    "#     adjusted_pvalues = fsInf.get(\"pv\")\n",
    "#     stop_step, false_discovery_rates = forward_stop_rule(adjusted_pvalues, alpha)\n",
    "#     adjusted_pvalues = adjusted_pvalues[:stop_step+1]\n",
    "#     rejections = np.sort((fsInf.get(\"vars\")-1).astype(np.int32)[:stop_step+1])\n",
    "#     if len(rejections) <= max_complexity:\n",
    "#         break\n",
    "# max_fdr = alpha\n",
    "# feature_names[rejections]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1daad6a-3b72-41de-973f-0a7482299969",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pre_top = X_pre[:, rejections]\n",
    "X_pre_top = X_pre_top/np.linalg.norm(X_pre_top, 2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af29d2b9-fd4a-45c9-a8bf-b02dcbf5a13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 4148.67it/s]\n"
     ]
    }
   ],
   "source": [
    "_, best_subsets = brute_force_all_subsets(X_pre_top, y_pre, max_support_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89342b11-612e-4129-9fba-cedcb268fdc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assume that mbics is a decreasing sequence\n",
    "complexities = np.array([len(_) for _ in best_subsets])\n",
    "\n",
    "if len(best_subsets) <= 2:\n",
    "    knee = complexities.max()\n",
    "else:\n",
    "    ebics = []\n",
    "    mbics = []\n",
    "    for _ in best_subsets:\n",
    "        loglik = log_like_value(X_pre_top[:, _]@np.linalg.lstsq(X_pre_top[:, _], y_pre, rcond=None)[0], \n",
    "                                y_pre)\n",
    "        ebics.append(ebic(loglik, len(_), len(y_pre), X_pre_top.shape[-1], const=0))\n",
    "        mbics.append(mbic(loglik, len(_), len(y_pre), X_pre_top.shape[-1], const=2))\n",
    "    ebics = np.array(ebics)\n",
    "    mbics = np.array(mbics)\n",
    "\n",
    "    if np.alltrue(np.array(mbics) >= np.array([max(mbics)+_*(min(mbics)-max(mbics))/(np.argmin(mbics)-np.argmax(mbics)) for _ in range(len(best_subsets))])):\n",
    "        knee = complexities.max()\n",
    "    else:    \n",
    "        decreasing_indices = np.array(mbics) <= np.array([max(mbics)+_*(min(mbics)-max(mbics))/(np.argmin(mbics)-np.argmax(mbics)) for _ in range(len(best_subsets))])\n",
    "        knee = knee_finder(mbics[decreasing_indices])\n",
    "        knee = (complexities[decreasing_indices])[knee]\n",
    "    \n",
    "knee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e27c59e-f740-4e66-92b7-e927a743b26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max fdr: 0.01\n",
      "1 0.0 0.0625\n",
      "2 5.963064764688218e-11 0.0625\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0); random.seed(0)\n",
    "n_samples = min(int(250*knee), len(y_pre))\n",
    "false_discovery_control_method = 'bh'\n",
    "print(\"max fdr:\", max_fdr)\n",
    "fdr_data = []\n",
    "for bs in best_subsets:\n",
    "    fdrs = []\n",
    "    for _ in range(len(y_pre)//n_samples):\n",
    "        X_test = X_pre_top[:, bs]\n",
    "        y_test = y_pre.ravel()\n",
    "        \n",
    "        np.random.seed(random.randint(0, 100))\n",
    "        # sample_indices = sorted(set([np.random.randint(len(y_pre)) for _ in range(n_samples)]))\n",
    "        sample_indices = fpsample.bucket_fps_kdline_sampling(X_test, n_samples=n_samples, h=3) # Farthest Point Sampling (FPS) is better!!!\n",
    "        X_test = X_test[sample_indices]; y_test = y_test[sample_indices]\n",
    "        # FPS + k-DPP\n",
    "        DPP = FiniteDPP('likelihood', **{'L': X_test.dot(X_test.T)})\n",
    "        DPP.flush_samples()\n",
    "        for _ in range(n_samples//(len(bs))):\n",
    "            DPP.sample_exact_k_dpp(size=len(bs))\n",
    "        sample_indices = np.unique(np.ravel(DPP.list_of_samples))\n",
    "        X_test = X_test[sample_indices]; y_test = y_test[sample_indices]\n",
    "        \n",
    "        manager = stepwise_selective_inference(support_size=X_test.shape[1])\n",
    "        M, p_list = manager.inference(X_test, y_test, np.std(y_test))\n",
    "        if false_discovery_control_method is not None:\n",
    "            p_list = stats.false_discovery_control(p_list, method=false_discovery_control_method)\n",
    "        # print(M, p_list, np.array(p_list) < 0.05)\n",
    "        fdrs.append(subset_fdr(p_list))\n",
    "        \n",
    "    fdrs = np.array(fdrs)\n",
    "    if fdrs.mean() < 1:\n",
    "        print(len(bs), fdrs.mean(), stats.wilcoxon(fdrs-max_fdr, alternative='less').pvalue)\n",
    "        fdr_data.append(fdrs)\n",
    "        \n",
    "fdr_data = np.array(fdr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77d997f5-ddf0-4c75-98ab-174feaa468d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AffinityPropagation, KMeans\n",
    "print(AffinityPropagation().fit(fdr_data).labels_)\n",
    "print(KMeans(n_clusters=2).fit(fdr_data).labels_)\n",
    "# plt.plot([1, 2, 3, 4], fdr_data.mean(axis=-1), 'o'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a7e323b-798f-4e98-9a8f-659083e7fca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max fdr: 0.01\n",
      "0.0 0.0625\n",
      "0.0 0.0625\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0); random.seed(0)\n",
    "n_samples = min(int(250*knee), len(X_pre))\n",
    "false_discovery_control_method = 'bh'\n",
    "print(\"max fdr:\", max_fdr)\n",
    "for bs in best_subsets:\n",
    "    fdrs = []\n",
    "    for _ in range(len(y_pre)//n_samples):\n",
    "        X_test = X_pre_top[:, bs]\n",
    "        y_test = y_pre.ravel()\n",
    "        \n",
    "        np.random.seed(random.randint(0, 100))\n",
    "        # sample_indices = sorted(set([np.random.randint(len(y_pre)) for _ in range(n_samples)]))\n",
    "        sample_indices = fpsample.bucket_fps_kdline_sampling(X_test, n_samples=n_samples, h=3) # Farthest Point Sampling (FPS) is better!!!\n",
    "        X_test = X_test[sample_indices]; y_test = y_test[sample_indices]\n",
    "        # FPS + k-DPP\n",
    "        # DPP = FiniteDPP('likelihood', **{'L': X_test.dot(X_test.T)})\n",
    "        # DPP.flush_samples()\n",
    "        # for _ in range(n_samples//(len(bs))):\n",
    "        #     DPP.sample_exact_k_dpp(size=len(bs))\n",
    "        # sample_indices = np.unique(np.ravel(DPP.list_of_samples))\n",
    "        # X_test = X_test[sample_indices]; y_test = y_test[sample_indices]\n",
    "        \n",
    "        manager = stepwise_selective_inference(support_size=X_test.shape[1])\n",
    "        M, p_list = manager.inference(X_test, y_test, np.std(y_test))\n",
    "        if false_discovery_control_method is not None:\n",
    "            p_list = stats.false_discovery_control(p_list, method=false_discovery_control_method)\n",
    "        # print(M, p_list, np.array(p_list) < 0.05)\n",
    "        fdrs.append(subset_fdr(p_list))\n",
    "        \n",
    "    fdrs = np.array(fdrs)\n",
    "    print(fdrs.mean(), stats.wilcoxon(fdrs-max_fdr, alternative='less').pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d199725d-a37e-4939-ba3e-45a9370e27e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bf1b8d-f7f2-4d83-94ca-731ec9f4cfd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sindy]",
   "language": "python",
   "name": "conda-env-sindy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
