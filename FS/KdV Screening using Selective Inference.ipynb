{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f83239b-bf12-480d-a091-268680d3344a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alibi is not installed in the environment.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from functools import partial\n",
    "from decimal import Decimal\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import pysindy as ps\n",
    "from tqdm import trange\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "from utils import *\n",
    "from solvel0 import solvel0, MIOSR\n",
    "from best_subset import backward_refinement, brute_force_all_subsets\n",
    "from UBIC import *\n",
    "from bayesian_model_evidence import log_evidence\n",
    "\n",
    "from skimage.restoration import estimate_sigma\n",
    "import bm3d\n",
    "from kneed import KneeLocator\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "\n",
    "from rdata import read_rds\n",
    "from selective_inference import forward_stop_rule\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import covariance\n",
    "from sklearn.linear_model import lars_path\n",
    "from abess import LinearRegression as AbessLinearRegression\n",
    "from knockpy import KnockoffFilter, knockoff_stats, knockoffs\n",
    "from knockpy.utilities import estimate_covariance\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from c2st.check import c2st # https://github.com/psteinb/c2st\n",
    "\n",
    "from mbic import mbic, mbic2, ebic\n",
    "\n",
    "from rdata import read_rds\n",
    "from selective_inference import forward_stop_rule, sfs_si, stepwise_selective_inference, subset_fdr\n",
    "import fpsample\n",
    "from dppy.finite_dpps import FiniteDPP\n",
    "\n",
    "from si4pipeline import (\n",
    "                        construct_pipelines, \n",
    "                        extract_features, \n",
    "                        initialize_dataset, \n",
    "                        intersection, \n",
    "                        lasso, \n",
    "                        marginal_screening, \n",
    "                        stepwise_feature_selection, \n",
    "                        union, \n",
    "                        PipelineManager\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e71a16eb-36ec-47c7-bfc7-da2d30480ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../Datasets/\"\n",
    "data = sio.loadmat(os.path.join(data_path, \"KdV_rudy.mat\"))\n",
    "u_clean = (data['usol']).real; u = u_clean.copy()\n",
    "x = data['x'].ravel()\n",
    "t = data['t'].ravel()\n",
    "xt = np.array([x, t], dtype=object)\n",
    "dt = t[1]-t[0]; dx = x[2]-x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83ed0fc0-ba29-427f-a921-44a584e38c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise level: 50.0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "noise_type = \"gaussian\"\n",
    "noise_lv = float(50)\n",
    "print(\"Noise level:\", noise_lv)\n",
    "noise = 0.01*np.abs(noise_lv)*(u.std())*np.random.randn(u.shape[0],u.shape[1])\n",
    "u = u + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d5a1158-9db5-4a81-85ae-5f49fcc1cbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "fake_noise = np.random.normal(loc=0.0, scale=estimate_sigma(u), size=u.shape)\n",
    "sigmas = estimate_sigma(u+fake_noise)*np.arange(0.1, 2., 0.1)\n",
    "est_sigma = sigmas[np.argmin([((u-bm3d.bm3d(u+fake_noise, sigma_psd=sigma, stage_arg=bm3d.BM3DStages.ALL_STAGES, blockmatches=(False, False)))**2).mean() \\\n",
    "                              for sigma in sigmas])]\n",
    "u = bm3d.bm3d(u, sigma_psd=est_sigma, \n",
    "                  stage_arg=bm3d.BM3DStages.ALL_STAGES, \n",
    "                  blockmatches=(False, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96244a07-bc2f-47f3-8919-fd1ad48354a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_poly = 6\n",
    "n_derivatives = 6\n",
    "n_weak = 2000\n",
    "function_library = ps.PolynomialLibrary(degree=n_poly, include_bias=False)\n",
    "\n",
    "np.random.seed(0)\n",
    "weak_lib = ps.WeakPDELibrary(\n",
    "    function_library=function_library,\n",
    "    derivative_order=n_derivatives,\n",
    "    spatiotemporal_grid=np.asarray([*np.meshgrid(x, t)]).T,\n",
    "    include_bias=True,\n",
    "    diff_kwargs={\"is_uniform\":True},\n",
    "    K=n_weak\n",
    ")\n",
    "\n",
    "X_pre = np.array(weak_lib.fit_transform(np.expand_dims(u, -1)))\n",
    "y_pre = weak_lib.convert_u_dot_integral(np.expand_dims(u, -1))\n",
    "feature_names = np.array(weak_lib.get_feature_names())\n",
    "\n",
    "### Cache ###\n",
    "# X_pre = np.load(\"../Cache/X_pre_kdv_noise50.npy\")\n",
    "# y_pre = np.load(\"../Cache/y_pre_kdv_noise50.npy\")\n",
    "# u_pre = y_pre.copy()\n",
    "# feature_names = np.load(\"../Cache/feature_names_kdv.npy\")\n",
    "# fsInf = read_rds(\"../R/R_data/fsInf_screening_kdv_noise50.rds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faeb246-0585-46a1-9636-e6787fafa06d",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c978314-9119-413c-9a13-e2bb1244c78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2026-04-04\n",
      "[ 7  9 13 36 11 25 15 18 26 37 27 38 30 35 39 42] [0.0, 0.0, 0.0, 5.743945542535656e-07, 0.0, 0.0, 0.0, 0.0, 0.0, 5.430100813441641e-13, 0.0, 4.440892098500626e-16, 0.0, 1.8068709543017647e-06, 1.2212453270876722e-15, 5.5067062021407764e-14] 1.4882924410348202e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.01,\n",
       " array(['x0_1', 'x0_111', 'x0_11111', 'x0x0_1', 'x0^3x0_1', 'x0^6x0_1',\n",
       "        'x0x0_111', 'x0^2x0_111', 'x0^3x0_111', 'x0^6x0_111',\n",
       "        'x0^5x0_1111', 'x0^6x0_1111', 'x0x0_11111', 'x0^2x0_11111',\n",
       "        'x0^3x0_11111', 'x0^6x0_11111'], dtype='<U13'),\n",
       " 16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_terms = 16\n",
    "max_complexity = 12\n",
    "alphas = [0.3, 0.25, 0.2, 0.15, 0.1, 0.05, 0.01]\n",
    "\n",
    "### OPTION-I ###\n",
    "# _, lars_p, _ = lars_path(StandardScaler().fit_transform(X_pre), y_pre.flatten(), method='lasso', alpha_min=1e-6, max_iter=1000)\n",
    "# lars_p = np.array(list(map(int, lars_p)))[:n_terms]\n",
    "\n",
    "### OPTION-II ###\n",
    "nonzero = np.nonzero(AbessLinearRegression(s_min=1, path_type='gs', fit_intercept=False, alpha=1e-9, max_iter=100).fit(X_pre, y_pre.flatten()).coef_)[0]\n",
    "nonzero = np.nonzero(MIOSR(X_pre, y_pre, alpha=1e-9, non_zero=min(len(nonzero), n_terms)))[0]\n",
    "_, lars_p, _ = lars_path(StandardScaler().fit_transform(X_pre[:, nonzero]), y_pre.flatten(), method='lasso', alpha_min=0)\n",
    "lars_p = nonzero[np.array(list(map(int, lars_p)))][:n_terms]\n",
    "\n",
    "X_test = X_pre[:, lars_p]\n",
    "sigma = np.std(y_pre-X_test@np.linalg.lstsq(X_test, y_pre)[0], ddof=1)\n",
    "manager = stepwise_selective_inference(support_size=len(lars_p))\n",
    "_, p_list = manager.inference(X_test, y_pre, sigma)\n",
    "print(lars_p, p_list, subset_fdr(p_list))\n",
    "\n",
    "for alpha in alphas:\n",
    "    adjusted_pvalues = p_list\n",
    "    stop_step, false_discovery_rates = forward_stop_rule(adjusted_pvalues, alpha)\n",
    "    adjusted_pvalues = adjusted_pvalues[:stop_step+1]\n",
    "    rejections = np.sort(lars_p[:stop_step+1])\n",
    "    if len(rejections) <= max_complexity: \n",
    "        break\n",
    "max_fdr = alpha\n",
    "max_fdr, feature_names[rejections], len(rejections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ae79a4-32b1-4396-b329-82b6092d8e33",
   "metadata": {},
   "source": [
    "### R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93dc47a4-a735-4811-bf69-5d6a21319401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_complexity = 12\n",
    "# alphas = [0.3, 0.2, 0.1, 0.05, 0.01]\n",
    "# for alpha in alphas:\n",
    "#     adjusted_pvalues = fsInf.get(\"pv\")\n",
    "#     stop_step, false_discovery_rates = forward_stop_rule(adjusted_pvalues, alpha)\n",
    "#     adjusted_pvalues = adjusted_pvalues[:stop_step+1]\n",
    "#     rejections = np.sort((fsInf.get(\"vars\")-1).astype(np.int32)[:stop_step+1])\n",
    "#     if len(rejections) <= max_complexity:\n",
    "#         break\n",
    "# max_fdr = alpha\n",
    "# feature_names[rejections]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1daad6a-3b72-41de-973f-0a7482299969",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pre_top = X_pre[:, rejections]\n",
    "X_pre_top = X_pre_top/np.linalg.norm(X_pre_top, 2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb48ee04-7a92-4bf2-9ec3-7f6f18438d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 8/8 [00:08<00:00,  1.12s/it]\n"
     ]
    }
   ],
   "source": [
    "_, best_subsets = brute_force_all_subsets(X_pre_top, y_pre, max_support_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d3b61b4-e383-4499-9195-e23d2811c577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assume that mbics is a decreasing sequence\n",
    "complexities = np.array([len(_) for _ in best_subsets])\n",
    "\n",
    "if len(best_subsets) <= 2:\n",
    "    knee = complexities.max()\n",
    "else:\n",
    "    ebics = []\n",
    "    mbics = []\n",
    "    for _ in best_subsets:\n",
    "        loglik = log_like_value(X_pre_top[:, _]@np.linalg.lstsq(X_pre_top[:, _], y_pre, rcond=None)[0], \n",
    "                                y_pre)\n",
    "        ebics.append(ebic(loglik, len(_), len(y_pre), X_pre_top.shape[-1], const=0))\n",
    "        mbics.append(mbic(loglik, len(_), len(y_pre), X_pre_top.shape[-1], const=2))\n",
    "    ebics = np.array(ebics)\n",
    "    mbics = np.array(mbics)\n",
    "\n",
    "    if np.alltrue(np.array(mbics) >= np.array([max(mbics)+_*(min(mbics)-max(mbics))/(np.argmin(mbics)-np.argmax(mbics)) for _ in range(len(best_subsets))])):\n",
    "        knee = complexities.max()\n",
    "    else:    \n",
    "        decreasing_indices = np.array(mbics) <= np.array([max(mbics)+_*(min(mbics)-max(mbics))/(np.argmin(mbics)-np.argmax(mbics)) for _ in range(len(best_subsets))])\n",
    "        knee = knee_finder(mbics[decreasing_indices])\n",
    "        knee = (complexities[decreasing_indices])[knee]\n",
    "    \n",
    "knee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e27c59e-f740-4e66-92b7-e927a743b26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max fdr: 0.01\n",
      "1 0.0 0.0625\n",
      "2 6.431109942823572e-05 0.0625\n",
      "3 0.2627757657676303 1.0\n",
      "4 0.37661688424135553 1.0\n",
      "5 0.4302632025045271 1.0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0); random.seed(0)\n",
    "n_samples = min(int(250*knee), len(y_pre))\n",
    "false_discovery_control_method = 'bh'\n",
    "print(\"max fdr:\", max_fdr)\n",
    "fdr_data = []\n",
    "for bs in best_subsets:\n",
    "    fdrs = []\n",
    "    for _ in range(len(y_pre)//n_samples):\n",
    "        X_test = X_pre_top[:, bs]\n",
    "        y_test = y_pre.ravel()\n",
    "        \n",
    "        np.random.seed(random.randint(0, 100))\n",
    "        # sample_indices = sorted(set([np.random.randint(len(y_pre)) for _ in range(n_samples)]))\n",
    "        sample_indices = fpsample.bucket_fps_kdline_sampling(X_test, n_samples=n_samples, h=3) # Farthest Point Sampling (FPS) is better!!!\n",
    "        X_test = X_test[sample_indices]; y_test = y_test[sample_indices]\n",
    "        # FPS + k-DPP\n",
    "        DPP = FiniteDPP('likelihood', **{'L': X_test.dot(X_test.T)})\n",
    "        DPP.flush_samples()\n",
    "        for _ in range(n_samples//(len(bs))):\n",
    "            DPP.sample_exact_k_dpp(size=len(bs))\n",
    "        sample_indices = np.unique(np.ravel(DPP.list_of_samples))\n",
    "        X_test = X_test[sample_indices]; y_test = y_test[sample_indices]\n",
    "        \n",
    "        manager = stepwise_selective_inference(support_size=X_test.shape[1])\n",
    "        M, p_list = manager.inference(X_test, y_test, np.std(y_test))\n",
    "        if false_discovery_control_method is not None:\n",
    "            p_list = stats.false_discovery_control(p_list, method=false_discovery_control_method)\n",
    "        # print(M, p_list, np.array(p_list) < 0.05)\n",
    "        fdrs.append(subset_fdr(p_list))\n",
    "        \n",
    "    fdrs = np.array(fdrs)\n",
    "    if fdrs.mean() < 1:\n",
    "        print(len(bs), fdrs.mean(), stats.wilcoxon(fdrs-max_fdr, alternative='less').pvalue)\n",
    "        fdr_data.append(fdrs)\n",
    "        \n",
    "fdr_data = np.array(fdr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "679438c1-b385-42dc-9037-8b47a14f39f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0]\n",
      "[0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AffinityPropagation, KMeans\n",
    "print(AffinityPropagation().fit(fdr_data).labels_)\n",
    "print(KMeans(n_clusters=2).fit(fdr_data).labels_)\n",
    "# plt.plot([1, 2, 3, 4], fdr_data.mean(axis=-1), 'o'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf9b3dd-7bca-4c2d-bceb-b761b8c6c4fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193c97b5-2586-4d2b-b4af-466cb8295d78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sindy]",
   "language": "python",
   "name": "conda-env-sindy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
